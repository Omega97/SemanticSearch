"path","filename","content"
".\prove_loader\images_captions.csv","images_captions.csv","1000268201_693b08cb0e.jpg,a little girl in a pink dress\n1001773457_577c3a7d70.jpg,two dogs playing on the road\n1002674143_1b742ab4b8.jpg,a child sitting in the grass\n1003163366_44323f5815.jpg,a woman laying on a bench\n1007129816_e794419615.jpg,man wearing a hat\n1007320043_627395c3d8.jpg,a young girl climbing on a rope\n1009434119_febe49276a.jpg,a dog running in a field\n1012212859_01547e3f17.jpg,a dog playing with a ball\n1015118661_980735411b.jpg,a young boy is walking down the street\n1015584366_dfcec3c85a.jpg,a dog is sitting on a log in the grass\n101654506_8eb26cfb60.jpg,a dog running in the snow"
".\prove_loader\prova.txt","prova.txt","hey come va?\nio bene, tu?"
".\prove_loader\vi_for_statisticians.pdf","vi_for_statisticians.pdf","Full Terms & Conditions of access and use can be found at\nhttp://www.tandfonline.com/action/journalInformation?journalCode=uasa20\nDownload by: [Australian Catholic University]\nDate: 19 July 2017, At: 08:54\nJournal of the American Statistical Association\nISSN: 0162-1459 (Print) 1537-274X (Online) Journal homepage: http://www.tandfonline.com/loi/uasa20\nVariational Inference: A Review for Statisticians\nDavid M. Blei, Alp Kucukelbir & Jon D. McAuliffe\nTo cite this article: David M. Blei, Alp Kucukelbir & Jon D. McAuliffe (2017) Variational Inference:\nA Review for Statisticians, Journal of the American Statistical Association, 112:518, 859-877, DOI:\n10.1080/01621459.2017.1285773\nTo link to this article:  http://dx.doi.org/10.1080/01621459.2017.1285773\nView supplementary material \nAccepted author version posted online: 27\nFeb 2017.\nPublished online: 27 Feb 2017.\nSubmit your article to this journal \nArticle views: 946\nView related articles \nView Crossmark data\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n, VOL. , NO. , –, Review\nhttps://doi.org/./..\nVariational Inference: A Review for Statisticians\nDavid M. Bleia, Alp Kucukelbirb, and Jon D. McAuliffec\naDepartment of Computer Science and Statistics, Columbia University, New York, NY; bDepartment of Computer Science, Columbia University, New\nYork, NY; cDepartment of Statistics, University of California, Berkeley, CA\nARTICLE HISTORY\nReceived January \nRevised December \nKEYWORDS\nAlgorithms; Computationally\nintensive methods; Statistical\ncomputing\nABSTRACT\nOne of the core problems of modern statistics is to approximate difficult-to-compute probability densities.\nThis problem is especially important in Bayesian statistics, which frames all inference about unknown quan-\ntities as a calculation involving the posterior density. In this article, we review variational inference (VI), a\nmethod from machine learning that approximates probability densities through optimization. VI has been\nused in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo\nsampling. The idea behind VI is to first posit a family of densities and then to find a member of that family\nwhich is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the\nideas behind mean-field variational inference, discuss the special case of VI applied to exponential family\nmodels, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochas-\ntic optimization to scale up to massive data. We discuss modern research in VI and highlight important\nopen problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze\nstatistical research on this class of algorithms. Supplementary materials for this article are available online.\n1. Introduction\nOne of the core problems of modern statistics is to approximate\ndifficult-to-compute probability densities. This problem is espe-\ncially important in Bayesian statistics, which frames all inference\nabout unknown quantities as a calculation about the posterior.\nModern Bayesian statistics relies on models for which the pos-\nterior is not easy to compute and corresponding algorithms for\napproximating them.\nIn this article, we review variational inference (VI), a method\nfrom machine learning for approximating probability densities\n(Jordan et al. 1999; Wainwright and Jordan 2008). Variational\ninference is widely used to approximate posterior densities for\nBayesian models, an alternative strategy to Markov chain Monte\nCarlo (MCMC) sampling. Compared to MCMC, variational\ninference tends to be faster and easier to scale to large data—it\nhas been applied to problems such as large-scale document\nanalysis, computational neuroscience, and computer vision.\nBut variational inference has been studied less rigorously than\nMCMC, and its statistical properties are less well understood.\nIn writing this article, our hope is to catalyze statistical research\non variational inference.\nFirst, we set up the general problem. Consider a joint density\nof latent variables z = z1:m and observations x = x1:n,\np(z, x) = p(z)p(x | z).\nIn Bayesian models, the latent variables help govern the\ndistribution of the data. A Bayesian model draws the\nlatent variables from a prior density p(z) and then relates\nthem to the observations through the likelihood p(x | z).\nCONTACT Alp Kucukelbir\nalp@cs.columbia.edu\nDepartment of Computer Science, Columbia University, New York, NY .\nColor versions of one or more of the ﬁgures in the article can be found online at www.tandfonline.com/r/JASA.\nSupplementary materials for this article are available online. Please go to www.tandfonline.com/r/JASA.\nInference in a Bayesian model amounts to conditioning\non data and computing the posterior p(z | x). In complex\nBayesian models, this computation often requires approximate\ninference.\nFor decades, the dominant paradigm for approximate infer-\nence has been MCMC (Hastings 1970; Gelfand and Smith 1990).\nIn MCMC, we first construct an ergodic Markov chain on z\nwhose stationary distribution is the posterior p(z | x). Then,\nwe sample from the chain to collect samples from the station-\nary distribution. Finally, we approximate the posterior with an\nempirical estimate constructed from (a subset of) the collected\nsamples.\nMCMC sampling has evolved into an indispensable tool\nto the modern Bayesian statistician. Landmark developments\ninclude the Metropolis–Hastings algorithm (Metropolis et al.\n1953; Hastings 1970), the Gibbs sampler (Geman and Geman\n1984), and its application to Bayesian statistics (Gelfand and\nSmith 1990). MCMC algorithms are under active investiga-\ntion. They have been widely studied, extended, and applied; see\nRobert and Casella (2004) for a perspective.\nHowever, there are problems for which we cannot easily use\nthis approach. These arise particularly when we need an approx-\nimate conditional faster than a simple MCMC algorithm can\nproduce, such as when datasets are large or models are very\ncomplex. In these settings, variational inference provides a good\nalternative approach to Bayesian inference.\nRather than use sampling, the main idea behind variational\ninference is to use optimization. First, we posit a family of\napproximate densities Q. This is a set of densities over the latent\nvariables. Then, we try to find the member of that family that\n© American Statistical Association\n860\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nminimizes the Kullback-Leibler (KL) divergence to the exact\nposterior,\nq∗(z) = arg min\nq(z)∈Q\nkl\n\nq(z)∥p(z | x)\n\n.\n(1)\nFinally, we approximate the posterior with the optimized mem-\nber of the family q∗(·).\nVariational inference thus turns the inference problem into\nan optimization problem, and the reach of the family Q man-\nages the complexity of this optimization. One of the key ideas\nbehind variational inference is to choose Q to be flexible enough\nto capture a density close to p(z | x), but simple enough for effi-\ncient optimization.1\nWe emphasize that MCMC and variational inference are dif-\nferent approaches to solving the same problem. MCMC algo-\nrithms sample a Markov chain; variational algorithms solve\nan optimization problem. MCMC algorithms approximate the\nposterior with samples from the chain; variational algorithms\napproximate the posterior with the result of the optimization.\nComparing variational inference and MCMC. When should\na statistician use MCMC and when should she use variational\ninference? We will offer some guidance. MCMC methods tend\nto be more computationally intensive than variational inference\nbut they also provide guarantees of producing (asymptotically)\nexact samples from the target density (Robert and Casella 2004).\nVariational inference does not enjoy such guarantees—it can\nonly find a density close to the target—but tends to be faster than\nMCMC. Because it rests on optimization, variational inference\neasily takes advantage of methods like stochastic optimization\n(Robbins and Monro 1951; Kushner and Yin 1997) and dis-\ntributed optimization. Some MCMC methods can also exploit\nthese innovations (Welling and Teh 2011; Ahmed et al. 2012).\nThus, variational inference is suited to large datasets and sce-\nnarios where we want to quickly explore many models; MCMC\nis suited to smaller datasets and scenarios where we happily pay a\nheavier computational cost for more precise samples. For exam-\nple, we might use MCMC in a setting where we spent 20 years\ncollecting a small but expensive dataset, where we are confi-\ndent that our model is appropriate, and where we require pre-\ncise inferences. We might use variational inference when fitting\na probabilistic model of text to one billion text documents and\nwhere the inferences will be used to serve search results to a\nlarge population of users. In this scenario, we can use distributed\ncomputation and stochastic optimization to scale and speed up\ninference, and we can easily explore many different models of\nthe data.\nDataset size is not the only consideration. Another factor\nis the geometry of the posterior distribution. For example,\nthe posterior of a mixture model has multiple modes, each\ncorresponding to a label permutation of the components. Gibbs\nsampling, if the model permits, is a powerful approach to\nsampling from such target distributions; it quickly focuses on\none of the modes. For mixture models where Gibbs sampling\nis not an option, variational inference may perform better\nWe focus here on KL(q||p)-based optimization, also called Kullback–Leibler varia-\ntional inference (Barber ). Wainwright and Jordan () emphasized that any\nprocedure that uses optimization to approximate a density can be termed “vari-\national inference.” This includes methods like expectation propagation (Minka\n), belief propagation (Yedidia, Freeman, and Weiss ), or even the Laplace\napproximation. We brieﬂy discuss alternative divergence measures in Section .\nthan a more general MCMC technique (e.g., Hamiltonian\nMonte Carlo), even for small datasets (Kucukelbir et al. 2015).\nExploring the interplay between model complexity and infer-\nence (and between variational inference and MCMC) is an\nexciting avenue for future research (see Section 5.4).\nThe relative accuracy of variational inference and MCMC is\nstill unknown. We do know that variational inference generally\nunderestimates the variance of the posterior density; this is a\nconsequence of its objective function. But, depending on the\ntask at hand, underestimating the variance may be acceptable.\nSeveral lines of empirical research have shown that variational\ninference does not necessarily suffer in accuracy, for exam-\nple, in terms of posterior predictive densities (Blei and Jordan\n2006; Braun and McAuliffe 2010; Kucukelbir et al. 2017); other\nresearch focuses on where variational inference falls short, espe-\ncially around the posterior variance, and tries to more closely\nmatch the inferences made by MCMC (Giordano, Broderick,\nand Jordan 2015). In general, a statistical theory and under-\nstanding around variational inference is an important open area\nof research (see Section 5.2). We can envision future results that\noutline which classes of models are particularly suited to each\nalgorithm and perhaps even theory that bounds their accuracy.\nMore broadly, variational inference is a valuable tool, alongside\nMCMC, in the statistician’s toolbox.\nIt might appear to the reader that variational inference is only\nrelevant to Bayesian analysis. Indeed, both variational inference\nand MCMC have had a significant impact on applied Bayesian\ncomputation and we will be focusing on latent variable Bayesian\nmodels here. We emphasize, however, that these techniques also\napply more generally to computation about intractable densities.\nMCMC is a tool for simulating from densities and variational\ninference is a tool for approximating densities. One need not be\na Bayesian to have use for variational inference.\nResearch on variational inference. The development of vari-\national techniques for Bayesian inference followed two parallel,\nyet separate, tracks. Peterson and Anderson (1987) is arguably\nthe first variational procedure for a particular model: a neu-\nral network. This article, along with insights from statistical\nmechanics (Parisi 1988), led to a flurry of variational inference\nprocedures for a wide class of models (Saul, Jaakkola, and Jordan\n1996; Jaakkola and Jordan 1996, 1997; Ghahramani and Jordan\n1997; Jordan et al. 1999). In parallel, Hinton and Van Camp\n(1993) proposed a variational algorithm for a similar neural\nnetwork model. Neal and Hinton (1998, first published in 1993)\nmade important connections to the expectation maximization\n(EM) algorithm (Dempster, Laird, and Rubin 1977), which then\nled to a variety of variational inference algorithms for other\ntypes of models (Waterhouse, MacKay, and Robinson 1996;\nMacKay 1997; Barber and Bishop 1998).\nModern research on variational inference focuses on sev-\neral aspects: tackling Bayesian inference problems that involve\nmassive data; using improved optimization methods for solving\nEquation (1) (which is usually subject to local minima); devel-\noping generic variational inference algorithms that are easy to\napply to a wide class of models; and increasing the accuracy of\nvariational inference, for example, by stretching the boundaries\nof Q while managing complexity in optimization.\nOrganization of this article. Section 2 describes the basic ideas\nbehind the simplest approach to variational inference: mean-\nfield inference and coordinate-ascent optimization. Section 3\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n861\nworks out the details for a Bayesian mixture of Gaussians, an\nexample model familiar to many readers. Sections 4.1 and 4.2\ndescribe variational inference for the class of models where\nthe joint density of the latent and observed variables is in\nthe exponential family—this includes many intractable models\nfrom modern Bayesian statistics and reveals deep connections\nbetween variational inference and the Gibbs sampler by Gelfand\nand Smith (1990). Section 4.3 expands on this algorithm to\ndescribe stochastic variational inference (Hoffman et al. 2013),\nwhich scales variational inference to massive data using stochas-\ntic optimization (Robbins and Monro 1951). Finally, with these\nfoundations in place, Section 5 gives a perspective on the field—\napplications in the research literature, a survey of theoretical\nresults, and an overview of some open problems.\n2. Variational Inference\nThe goal of variational inference is to approximate a conditional\ndensity of latent variables given observed variables. The key idea\nis to solve this problem with optimization. We use a family of\ndensities over the latent variables, parameterized by free “vari-\national parameters.” The optimization finds the member of this\nfamily, that is, the setting of the parameters, which is closest in\nKL divergence to the conditional of interest. The fitted varia-\ntional density then serves as a proxy for the exact conditional\ndensity. (All vectors defined below are column vectors, unless\nstated otherwise.)\n2.1. The Problem of Approximate Inference\nLet x = x1:n be a set of observed variables and z = z1:m be a set\nof latent variables, with joint density p(z, x). We omit constants,\nsuch as hyperparameters, from the notation.\nThe inference problem is to compute the conditional density\nof the latent variables given the observations, p(z | x). This con-\nditional can be used to produce point or interval estimates of the\nlatent variables, form predictive densities of new data, and more.\nWe can write the conditional density as\np(z | x) = p(z, x)\np(x) .\n(2)\nThe denominator contains the marginal density of the observa-\ntions, also called the evidence. We calculate it by marginalizing\nout the latent variables from the joint density,\np(x) =\n\np(z, x) dz.\n(3)\nFor many models, this evidence integral is unavailable in closed\nform or requires exponential time to compute. The evidence is\nwhat we need to compute the conditional from the joint; this is\nwhy inference in such models is hard.\nNote we assume that all unknown quantities of interest are\nrepresented as latent random variables. This includes parame-\nters that might govern all the data, as found in Bayesian models,\nand latent variables that are “local” to individual data points.\nBayesian mixture of Gaussians. Consider a Bayesian mixture\nof unit-variance univariate Gaussians. There are K mixture com-\nponents, corresponding to K Gaussian distributions with means\nμ = {μ1, . . . , μK}. The mean parameters are drawn indepen-\ndently from a common prior p(μk), which we assume to be\na Gaussian N (0, σ 2); the prior variance σ 2 is a hyperparam-\neter. To generate an observation xi from the model, we first\nchoose a cluster assignment ci. It indicates which latent clus-\nter xi comes from and is drawn from a categorical distribution\nover {1, . . . , K}. (We encode ci as an indicator K-vector, all zeros\nexcept for a one in the position corresponding to xi’s cluster.) We\nthen draw xi from the corresponding Gaussian N (c⊤\ni μ, 1).\nThe full hierarchical model is\nμk ∼N (0, σ 2),\nk = 1, . . . , K, (4)\nci ∼categorical(1/K, . . . , 1/K),\ni = 1, . . . , n, (5)\nxi | ci, μ ∼N\n\nc⊤\ni μ, 1\n\ni = 1, . . . , n. (6)\nFor a sample of size n, the joint density of latent and observed\nvariables is\np(μ, c, x) = p(μ)\nn\n\ni=1\np(ci)p(xi | ci, μ).\n(7)\nThe latent variables are z = {μ, c}, the K class means and n class\nassignments.\nHere, the evidence is\np(x) =\n\np(μ)\nn\n\ni=1\n\nci\np(ci)p(xi | ci, μ) dμ.\n(8)\nThe integrand in Equation (8) does not contain a separate fac-\ntor for each μk. (Indeed, each μk appears in all n factors of the\nintegrand.) Thus, the integral in Equation (8) does not reduce to\na product of one-dimensional integrals over the μk’s. The time\ncomplexity of numerically evaluating the K-dimensional inte-\ngral is O(Kn).\nIf we distribute the product over the sum in (8) and rearrange,\nwe can write the evidence as a sum over all possible configura-\ntions c of cluster assignments,\np(x) =\n\nc\np(c)\n\np(μ)\nn\n\ni=1\np(xi | ci, μ) dμ.\n(9)\nHere, each individual integral is computable, thanks to the con-\njugacy between the Gaussian prior on the components and the\nGaussian likelihood. But there are Kn of them, one for each con-\nfiguration of the cluster assignments. Computing the evidence\nremains exponential in K, hence intractable.\n2.2. The Evidence Lower Bound\nIn variational inference, we specify a family Q of densities over\nthe latent variables. Each q(z) ∈Q is a candidate approxima-\ntion to the exact conditional. Our goal is to find the best candi-\ndate, the one closest in KL divergence to the exact conditional.2\nInference now amounts to solving the following optimization\nproblem,\nq∗(z) = arg min\nq(z)∈Q\nkl\n\nq(z)∥p(z | x)\n\n.\n(10)\nThe KL divergence is an information-theoretical measure of proximity between\ntwo densities. It is asymmetric—that is, KL(q∥p) ̸= KL(p∥q)—and nonnegative.\nIt is minimized when q(·) = p(·).\n862\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nOnce found, q∗(·) is the best approximation of the conditional,\nwithin the family Q. The complexity of the family determines\nthe complexity of this optimization.\nHowever, this objective is not computable because it requires\ncomputing the logarithm of the evidence, log p(x) in Equation\n(3). (That the evidence is hard to compute is why we appeal to\napproximate inference in the first place.) To see why, recall that\nKL divergence is\nkl\n\nq(z)∥p(z | x)\n\n= E\n\nlog q(z)\n\n−E\n\nlog p(z | x)\n\n, (11)\nwhere all expectations are taken with respect to q(z). Expanding\nthe conditional,\nkl\n\nq(z)∥p(z | x)\n\n= E[log q(z)] −E[log p(z, x)]\n+ log p(x).\n(12)\nThis reveals its dependence on log p(x).\nBecause we cannot compute the KL, we optimize an alter-\nnative objective that is equivalent to the KL up to an added\nconstant,\nelbo(q) = E\n\nlog p(z, x)\n\n−E\n\nlog q(z)\n\n.\n(13)\nThis function is called the evidence lower bound (ELBO) (for\nreasons explained in the text following). The ELBO is the neg-\native KL divergence of Equation (12) plus log p(x), which is a\nconstant with respect to q(z). Maximizing the ELBO is equiva-\nlent to minimizing the KL divergence.\nExamining the ELBO gives intuitions about the optimal vari-\national density. We rewrite the ELBO as a sum of the expected\nlog-likelihood of the data and the KL divergence between the\nprior p(z) and q(z),\nelbo(q) = E\n\nlog p(z)\n\n+ E\n\nlog p(x | z)\n\n−E\n\nlog q(z)\n\n= E\n\nlog p(x | z)\n\n−kl\n\nq(z)∥p(z)\n\n.\nWhich values of z will this objective encourage q(z) to place its\nmass on? The first term is an expected likelihood; it encourages\ndensities that place their mass on configurations of the latent\nvariables that explain the observed data. The second term is\nthe negative divergence between the variational density and the\nprior; it encourages densities close to the prior. Thus, the vari-\national objective mirrors the usual balance between likelihood\nand prior.\nAnother property of the ELBO is that it lower-bounds the\n(log) evidence, log p(x) ≥elbo(q) for any q(z). This explains\nthe name. To see this notice that Equations (12) and (13) give\nthe following expression of the evidence,\nlog p(x) = kl\n\nq(z)∥p(z | x)\n\n+ elbo(q).\n(14)\nThe bound then follows from the fact that kl (·) ≥0 (Kullback\nand Leibler 1951). In the original literature on variational infer-\nence, this was derived through Jensen’s inequality (Jordan et al.\n1999).\nThe relationship between the ELBO and log p(x) has led to\nusing the variational bound as a model selection criterion. This\nhas been explored for mixture models (Ueda and Ghahramani\n2002; McGrory and Titterington 2007) and more generally\n(Beal and Ghahramani 2003). The premise is that the bound is a\ngood approximation of the marginal likelihood, which provides\na basis for selecting a model. Though this sometimes works in\npractice, selecting based on a bound is not justified in theory.\nOther research has used variational approximations in the log\npredictive density to use VI in cross-validation-based model\nselection (Nott et al. 2012).\nFinally, many readers will notice that the first term of the\nELBO in Equation (13) is the expected complete log-likelihood,\nwhich is optimized by the EM algorithm (Dempster, Laird, and\nRubin 1977). The EM algorithm was designed for finding maxi-\nmum likelihood estimates in models with latent variables. It uses\nthe fact that the ELBO is equal to the log-marginal-likelihood\nlog p(x) (i.e., the log evidence) when q(z) = p(z | x). EM alter-\nnates between computing the expected complete log-likelihood\naccording to p(z | x) (the E step) and optimizing it with respect\nto the model parameters (the M step). Unlike variational infer-\nence, EM assumes the expectation under p(z | x) is computable\nand uses it in otherwise difficult parameter estimation problems.\nUnlike EM, variational inference does not estimate fixed model\nparameters—it is often used in a Bayesian setting where classical\nparameters are treated as latent variables. Variational inference\napplies to models where we cannot compute the exact condi-\ntional of the latent variables.3\n2.3. The Mean-Field Variational Family\nWe described the ELBO, the variational objective function in the\noptimization of Equation (10). We now describe a variational\nfamily Q, to complete the specification of the optimization prob-\nlem. The complexity of the family determines the complexity of\nthe optimization; it is more difficult to optimize over a complex\nfamily than a simple family.\nIn this review, we focus on the mean-field variational family,\nwhere the latent variables are mutually independent and each\ngoverned by a distinct factor in the variational density. A generic\nmember of the mean-field variational family is\nq(z) =\nm\n\nj=1\nq j(z j).\n(15)\nEach latent variable zj is governed by its own variational factor,\nthe density qj(z j). In optimization, these variational factors are\nchosen to maximize the ELBO of Equation (13).\nWe emphasize that the variational family is not a model of the\nobserved data—indeed, the data x does not appear in Equation\n(15). Instead, it is the ELBO, and the corresponding KL mini-\nmization problem, which connects the fitted variational density\nto the data and model.\nNotice we have not specified the parametric form of the indi-\nvidual variational factors. In principle, each can take on any\nparametric form appropriate to the corresponding random vari-\nable. For example, a continuous variable might have a Gaussian\nfactor; a categorical variable will typically have a categorical fac-\ntor. We will see in Sections 4, 4.1, and 4.2 that there are many\nTwonotes:(a)VariationalEMistheEMalgorithmwithavariationalE-step,thatis,a\ncomputation of an approximate conditional. (b) The coordinate ascent algorithm\nof Section .resembles the EM algorithm. The “E step” computes approximate\nconditionals of local latent variables; the “M step”computes a conditional of the\nglobal latent variables.\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n863\nmodels for which properties of the model determine optimal\nforms of the mean-field variational factors qj(z j).\nFinally, though we focus on mean-field inference in this\nreview, researchers have also studied more complex families.\nOne way to expand the family is to add dependencies between\nthe variables (Saul and Jordan 1996; Barber and Wiegerinck\n1999); this is called structured variational inference. Another\nway to expand the family is to consider mixtures of variational\ndensities, that is, additional latent variables within the varia-\ntional family (Bishop et al. 1998). Both of these methods poten-\ntially improve the fidelity of the approximation, but there is\na trade off. Structured and mixture-based variational families\ncome with a more difficult-to-solve variational optimization\nproblem.\nBayesian mixture of Gaussians (continued). Consider again\nthe Bayesian mixture of Gaussians. The mean-field variational\nfamily contains approximate posterior densities of the form\nq(μ, c) =\nK\n\nk=1\nq\n\nμk; mk, s2\nk\n\nn\n\ni=1\nq(ci; ϕi).\n(16)\nFollowing the mean-field recipe, each latent variable is governed\nby its own variational factor. The factor q(μk; mk, s2\nk) is a Gaus-\nsian distribution on the kth mixture component’s mean param-\neter; its mean is mk and its variance is s2\nk. The factor q(ci; ϕi) is\na distribution on the ith observation’s mixture assignment; its\nassignment probabilities are a K-vector ϕi.\nHere, we have asserted parametric forms for these factors:\nthe mixture components are Gaussian with variational param-\neters (mean and variance) specific to the kth cluster; the cluster\nassignments are categorical with variational parameters (cluster\nprobabilities) specific to the ith data point. In fact, these are the\noptimal forms of the mean-field variational density for the mix-\nture of Gaussians.\nWith the variational family in place, we have completely\nspecified the variational inference problem for the mixture of\nGaussians. The ELBO is defined by the model definition in\nEquation (7) and the mean-field family in Equation (16). The\ncorresponding variational optimization problem maximizes\nthe ELBO with respect to the variational parameters, that is,\nthe Gaussian parameters for each mixture component and the\ncategorical parameters for each cluster assignment. We will see\nthis example through in Section 3.\nVisualizing the mean-field approximation. The mean-field\nfamily is expressive because it can capture any marginal density\nof the latent variables. However, it cannot capture correlation\nbetween them. Seeing this in action reveals some of the intu-\nitions and limitations of mean-field variational inference.\nConsider a two-dimensional Gaussian distribution, shown in\nviolet in Figure 1. This density is highly correlated, which defines\nits elongated shape.\nThe optimal mean-field variational approximation to this\nposterior is a product of two Gaussian distributions. Figure 1\nshows the mean-field variational density after maximizing the\nELBO. While the variational approximation has the same mean\nas the original density, its covariance structure is, by construc-\ntion, decoupled.\nFurther, the marginal variances of the approximation under-\nrepresent those of the target density. This is a common effect\nFigure . Visualizing the mean-ﬁeld approximation to a two-dimensional Gaussian\nposterior. The ellipses show the eﬀect of mean-ﬁeld factorization. (The ellipses are\n2σ contours of the Gaussian distributions.)\nin mean-field variational inference and, with this example, we\ncan see why. The KL divergence from the approximation to the\nposterior is in Equation (11). It penalizes placing mass in q(·) on\nareas where p(·) has little mass, but penalizes less the reverse. In\nthis example, to successfully match the marginal variances, the\ncircular q(·) would have to expand into territory where p(·) has\nlittle mass.\n2.4. Coordinate Ascent Mean-Field Variational Inference\nUsing the ELBO and the mean-field family, we have cast approx-\nimate conditional inference as an optimization problem. In this\nsection, we describe one of the most commonly used algorithms\nfor solving this optimization problem, coordinate ascent vari-\national inference (CAVI) (Bishop 2006). CAVI iteratively opti-\nmizes each factor of the mean-field variational density, while\nholding the others fixed. It climbs the ELBO to a local optimum.\nThe algorithm. We first state a result. Consider the jth latent\nvariable z j. The complete conditional of zj is its conditional den-\nsity given all of the other latent variables in the model and\nthe observations, p(z j | z−j, x). Fix the other variational factors\nqℓ(zℓ), ℓ̸= j. The optimal qj(z j) is then proportional to the\nexponentiated expected log of the complete conditional,\nq∗\nj(z j) ∝exp{E−j[log p(z j | z−j, x)]}.\n(17)\nThe expectation in Equation (17) is with respect to the (currently\nfixed) variational density over z−j, that is, 	\nℓ̸=j qℓ(zℓ). Equiva-\nlently, Equation (17) is proportional to the exponentiated log of\nthe joint,\nq∗\nj(z j) ∝exp{E−j[log p(z j, z−j, x)]}.\n(18)\nBecause of the mean-field property—all the latent variables are\nindependent—the expectations on the right-hand side do not\ninvolve the jth variational factor. Thus, this is a valid coordinate\nupdate.\nThese equations underlie the CAVI algorithm, presented as\nAlgorithm 1. We maintain a set of variational factors qℓ(zℓ).\nWe iterate through them, updating qj(z j) using Equation (18).\nCAVI goes uphill on the ELBO of Equation (13), eventually find-\ning a local optimum. As examples we show CAVI for a mixture\nof Gaussians in Section 3 and for a nonconjugate linear regres-\nsion in Appendix A (in the online supplementary materials).\nCAVI can also be seen as a “message passing” algorithm\n(Winn and Bishop 2005), iteratively updating each random vari-\nable’s variational parameters based on the variational param-\neters of the variables in its Markov blanket. This perspective\n864\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nenabled the design of automated software for a large class of\nmodels (Wand et al. 2011; Minka et al. 2014). Variational mes-\nsage passing connects variational inference to the classical theo-\nries of graphical models and probabilistic inference (Pearl 1988;\nLauritzen and Spiegelhalter 1988). It has been extended to non-\nconjugate models (Knowles and Minka 2011) and generalized\nvia factor graphs (Minka 2005).\nAlgorithm 1: Coordinate ascent variational inference\n(CAVI)\nInput: A model p(x, z), a data set x\nOutput: A variational density q(z) = 	m\nj=1 q j(z j)\nInitialize: Variational factors qj(z j)\nwhile the ELBO has not converged do\nfor j ∈{1, . . . , m} do\nSet q j(z j) ∝exp{E−j[log p(z j | z−j, x)]}\nend\nCompute elbo(q) = E[log p(z, x)] + E[log q(z)]\nend\nreturn q(z)\nFinally, CAVI is closely related to Gibbs sampling (Geman\nand Geman 1984; Gelfand and Smith 1990), the classical\nworkhorse of approximate inference. The Gibbs sampler main-\ntains a realization of the latent variables and iteratively sam-\nples from each variable’s complete conditional. Equation (18)\nuses the same complete conditional. It takes the expected log,\nand uses this quantity to iteratively set each variable’s variational\nfactor.4\nDerivation. We now derive the coordinate update in Equation\n(18). The idea appears in Bishop (2006), but the argument there\nuses gradients, which we do not. Rewrite the ELBO of Equation\n(13) as a function of the jth variational factor qj(z j), absorbing\ninto a constant the terms that do not depend on it,\nelbo(qj) = E j[E−j[log p(z j, z−j, x)]]−E j[log q j(z j)]+\nconst. (19)\nWe have rewritten the first term of the ELBO using iterated\nexpectation. The second term we have decomposed, using the\nindependence of the variables (i.e., the mean-field assumption)\nand retaining only the term that depends on qj(z j).\nUp to an added constant, the objective function in Equation\n(19) is equal to the negative KL divergence between qj(z j) and\nq∗\nj(z j) from Equation (18). Thus, we maximize the ELBO with\nrespect to qj when we set q j(z j) = q∗\nj(z j).\n2.5. Practicalities\nHere, we highlight a few things to keep in mind when imple-\nmenting and using variational inference in practice.\nInitialization. The ELBO is (generally) a nonconvex objec-\ntive function. CAVI only guarantees convergence to a local opti-\nmum, which can be sensitive to initialization. Figure 2 shows\nthe ELBO trajectory for 10 random initializations using the\nMany readers will know that we can signiﬁcantly speed up the Gibbs sampler\nby marginalizing out some of the latent variables; this is called collapsed Gibbs\nsampling. We can speed up variational inference with similar reasoning; this is\ncalled collapsed variational inference. It has been developed for the same class\nof models described here (Sung, Ghahramani, and Bang ; Hensman, Rattray,\nand Lawrence ). These ideas are outside the scope of our review.\nFigure . Diﬀerent initializations may lead CAVI to ﬁnd diﬀerent local optima of the\nELBO.\nGaussian mixture model. The means of the variational factors\nwere randomly initialized by drawing from a factorized Gaus-\nsian calibrated to the empirical mean and variance of the dataset.\n(This inference is on images; see Section 3.4.) Each initialization\nreaches a different value, indicating the presence of many local\noptima in the ELBO. In terms of KL(q||p), better local optima\ngive variational densities that are closer to the exact posterior.\nThis is not always a disadvantage. Some models, such as\nthe mixture of Gaussians (Section 3 and Appendix B, in\nthe online supplementary materials) and mixed-membership\nmodel (Appendix C, in the online supplementary materials),\nexhibit many posterior modes due to label switching: swap-\nping cluster assignment labels induces many symmetric poste-\nrior modes. Representing one of these modes is sufficient for\nexploring latent clusters or predicting new observations.\nAssessing convergence. Monitoring the ELBO in CAVI is\nsimple; we typically declare convergence once the change in\nELBO falls below some small threshold. However, computing\nthe ELBO of the full dataset may be undesirable. Instead, we\nsuggest computing the average log predictive of a small held-\nout dataset. Monitoring changes here is a proxy to monitoring\nthe ELBO of the full data. (Unlike the full ELBO, held-out pre-\ndictive probability is not guaranteed to monotonically increase\nacross iterations of CAVI.)\nNumerical stability. Probabilities are constrained to live\nwithin [0, 1]. Precisely manipulating and performing arithmetic\non small numbers requires additional care. When possible, we\nrecommend working with logarithms of probabilities. One use-\nful identity is the “log-sum-exp” trick,\nlog\n\n \ni\nexp(xi)\n\n= α + log\n\n \ni\nexp(xi −α)\n\n.\n(20)\nThe constant α is typically set to maxi xi. This provides numer-\nical stability to common computations in variational inference\nprocedures.\n3. A Complete Example: Bayesian Mixture\nof Gaussians\nAs an example, we return to the simple mixture of Gaussians\nmodel of Section 2.1. To review, consider K mixture compo-\nnents and n real-valued data points x1:n. The latent variables\nare K real-valued mean parameters μ = μ1:K and n latent-class\nassignments c = c1:n. The assignment ci indicates which latent\ncluster xi comes from. In detail, ci is an indicator K-vector,\nall zeros except for a one in the position corresponding to\nxi’s cluster. There is a fixed hyperparameter σ 2, the variance\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n865\nof the normal prior on the μk’s. We assume the observation\nvariance is one and take a uniform prior over the mixture\ncomponents.\nThe joint density of the latent and observed variables is in\nEquation (7). The variational family is in Equation (16). Recall\nthat there are two types of variational parameters—categorical\nparameters ϕi for approximating the posterior cluster assign-\nment of the ith data point and Gaussian parameters mk and s2\nk\nfor approximating the posterior of the kth mixture component.\nWe combine the joint and the mean-field family to form the\nELBO for the mixture of Gaussians. It is a function of the varia-\ntional parameters m, s2, and ϕ,\nelbo(m, s2, ϕ) =\nK\n\nk=1\nE\n\nlog p(μk); mk, s2\nk\n\n+\nn\n\ni=1\n\nE\n\nlog p(ci); ϕi\n\n+ E\n\nlog p(xi | ci, μ); ϕi, m, s2\n−\nn\n\ni=1\nE\n\nlog q\n\nci; ϕi\n\n−\nK\n\nk=1\nE\n\nlog q(μk; mk, s2\nk)\n\n.\n(21)\nIn each term, we have made explicit the dependence on the vari-\national parameters. Each expectation can be computed in closed\nform.\nThe CAVI algorithm updates each variational parameter in\nturn. We first derive the update for the variational cluster assign-\nment factor; we then derive the update for the variational mix-\nture component factor.\n3.1. The Variational Density of the Mixture Assignments\nWe first derive the variational update for the cluster assignment\nci. Using Equation (18),\nq∗(ci; ϕi) ∝exp{log p(ci) + E[log p(xi | ci, μ); m, s2]}.\n(22)\nThe terms in the exponent are the components of the joint den-\nsity that depend on ci. The expectation in the second term is over\nthe mixture components μ.\nThe first term of Equation (22) is the log prior of ci. It is the\nsame for all possible values of ci, log p(ci) = −log K. The sec-\nond term is the expected log of the cith Gaussian density. Recall-\ning that ci is an indicator vector, we can write\np(xi | ci, μ) =\nK\n\nk=1\np(xi | μk)cik.\nWe use this to compute the expected log probability,\nE\n\nlog p(xi | ci, μ)\n\n=\n\nk\ncikE\n\nlog p(xi | μk); mk, s2\nk\n\n(23)\n=\n\nk\ncikE\n\n−(xi −μk)2\n2; mk, s2\nk\n\n+ const.\n(24)\n=\n\nk\ncik\n\nE\n\nμk; mk, s2\nk\n\nxi −E\n\nμ2\nk; mk, s2\nk\n \n2\n\n+ const.\n(25)\nIn each line we remove terms that are constant with respect to\nci. This calculation requires E [μk] and E[μ2\nk] for each mixture\ncomponent, both computable from the variational Gaussian on\nthe kth mixture component.\nThus, the variational update for the ith cluster assignment is\nϕik ∝exp\n\nE\n\nμk; mk, s2\nk\n\nxi −E\n\nμ2\nk; mk, s2\nk\n \n2\n\n.\n(26)\nNotice it is only a function of the variational parameters for the\nmixture components.\n3.2. The Variational Density of the\nMixture-Component Means\nWe turn to the variational density q(μk; mk, s2\nk) of the kth mix-\nture component. Again we use Equation (18) and write down\nthe joint density up to a normalizing constant,\nq(μk) ∝exp\n\nlog p(μk) +\nn\n\ni=1\nE\n\nlog p(xi | ci, μ); ϕi, m−k, s2\n−k\n\n\n.\n(27)\nWe now calculate the unnormalized logarithm of this\ncoordinate-optimal q(μk). Recall ϕik is the probability that\nthe ith observation comes from the kth cluster. Because ci is an\nindicator vector, we see that ϕik = E [cik; ϕi]. Now\nlog q(μk)\n= log p(μk)+\n\ni\nE\n\nlog p(xi|ci, μ); ϕi, m−k, s2\n−k\n\n+const.\n(28)\n= log p(μk) +\n\ni\nE\n\ncik log p(xi | μk); ϕi\n\n+ const.\n(29)\n= −μ2\nk/2σ 2 +\n\ni\nE [cik; ϕi] log p(xi | μk) + const.\n(30)\n= −μ2\nk/2σ 2 +\n\ni\nϕik\n\n−(xi −μk)2/2\n\n+ const.\n(31)\n= −μ2\nk/2σ 2 +\n\ni\nϕikxiμk −ϕikμ2\nk/2 + const.\n(32)\n=\n \ni\nϕikxi\n\nμk −\n\n1/2σ 2 +\n\ni\nϕik/2\n\nμ2\nk + const. (33)\nThis calculation reveals that the coordinate-optimal variational\ndensity of μk is an exponential family with sufficient statis-\ntics {μk, μ2\nk} and natural parameters {n\ni=1 ϕikxi, −1/2σ 2 −\nn\ni=1 ϕik/2}, that is, a Gaussian. Expressed in terms of the vari-\national mean and variance, the updates for q(μk) are\nmk =\n\ni ϕikxi\n1/σ 2 + \ni ϕik\n,\ns2\nk =\n1\n1/σ 2 + \ni ϕik\n.\n(34)\nThese updates relate closely to the complete conditional\ndensity of the kth component in the mixture model. The com-\nplete conditional is a posterior Gaussian given the data assigned\nto the kth component. The variational update is a weighted\ncomplete conditional, where each data point is weighted by its\nvariational probability of being assigned to component k.\n866\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nAlgorithm 2: CAVI for a Gaussian mixture model\nInput: Data x1:n, number of components K, prior variance\nof component means σ 2\nOutput: Variational densities q(μk; mk, s2\nk) (Gaussian)\nand q(zi; ϕi) (K-categorical)\nInitialize: Variational parameters m = m1:K, s2 = s2\n1:K,\nand ϕ = ϕ1:n\nwhile the ELBO has not converged do\nfor i ∈{1, . . . , n} do\nSet ϕik ∝exp{E[μk; mk, s2\nk]xi −E[μ2\nk; mk, s2\nk]/2}\nend\nfor k ∈{1, . . . , K} do\nSet mk ←−\n\ni ϕikxi\n1/σ 2 + \ni ϕik\nSet s2\nk ←−\n1\n1/σ 2 + \ni ϕik\nend\nCompute elbo(m, s2, ϕ)\nend\nreturn q(m, s2, ϕ)\n3.3. CAVI for the Mixture of Gaussians\nAlgorithm 2 presents coordinate-ascent variational inference\nfor the Bayesian mixture of Gaussians. It combines the varia-\ntional updates in Equation (22) and Equation (34). The algo-\nrithm requires computing the ELBO of Equation (21). We use\nthe ELBO to track the progress of the algorithm and assess when\nit has converged.\nOnce we have a fitted variational density, we can use it as we\nwould use the posterior. For example, we can obtain a poste-\nrior decomposition of the data. We assign points to their most\nlikely mixture assignment ˆci = arg maxk ϕik and estimate cluster\nmeans with their variational means mk.\nWe can also use the fitted variational density to approximate\nthe predictive density of new data. This approximate predictive\nis a mixture of Gaussians,\np(xnew | x1:n) ≈1\nK\nK\n\nk=1\np(xnew | mk),\n(35)\nwhere p(xnew | mk) is a Gaussian with mean mk and unit\nvariance.\n3.4. Empirical Study\nWe present two analyses to demonstrate the mixture of Gaus-\nsians algorithm in action. The first is a simulation study; the\nsecond is an analysis of a dataset of natural images.\nSimulation study. Consider two-dimensional real-valued data\nx. We simulate K = 5 Gaussians with random means, covari-\nances, and mixture assignments. Figure 3 shows the data;\neach point is colored according to its true cluster. Figure 3\nalso illustrates the initial variational density of the mixture\ncomponents—each is a Gaussian, nearly centered, and with a\nwide variance; the subpanels plot the variational density of the\ncomponents as the CAVI algorithm progresses.\nThe progression of the ELBO tells a story. We highlight key\npoints where the ELBO develops “elbows,” phases of the maxi-\nmization where the variational approximation changes its shape.\nThese “elbows” arise because the ELBO is not a convex function\nin terms of the variational parameters; CAVI iteratively reaches\nbetter plateaus.\nFinally, we plot the logarithm of the Bayesian predictive den-\nsity as approximated by the variational density. Here, we report\nthe average across held-out data. Note this plot is smoother than\nthe ELBO.\nImage analysis. We now turn to an experimental study.\nConsider the task of grouping images according to their color\nprofiles. One approach is to compute the color histogram of\nthe images. Figure 4 shows the red, green, and blue channel\nhistograms of two images from the imageclef data (Villegas,\nParedes, and Thomee 2013). Each histogram is a vector of\nlength 192; concatenating the three color histograms gives a\n576-dimensional representation of each image, regardless of its\noriginal size in pixel-space.\nWe use CAVI to fit a Gaussian mixture model with 30 clusters\nto image histograms. We randomly select two sets of 10,000\nimages from the imageclef collection to serve as training\nand testing datasets. Figure 5 shows similarly colored images\nassigned to four randomly chosen clusters. Figure 6 shows the\naverage log predictive accuracy of the testing set as a function\nof time. We compare CAVI to an implementation in Stan (Stan\nDevelopment Team 2015), which uses a Hamiltonian Monte\nCarlo-based sampler (Hoffman and Gelman 2014). (Details are\nin Appendix B.) CAVI is orders of magnitude faster than this\nsampling algorithm.5\n4. Variational Inference with Exponential Families\nWe described mean-field variational inference and derived\nCAVI, a general coordinate-ascent algorithm for optimizing the\nELBO. We demonstrated this approach on a simple mixture of\nGaussians, where each coordinate update was available in closed\nform.\nThe mixture of Gaussians is one member of the important\nclass of models where each complete conditional is in the expo-\nnential family. This includes a number of widely used mod-\nels, such as Bayesian mixtures of exponential families, factorial\nmixture models, matrix factorization models, certain hierarchi-\ncal regression models (e.g., linear regression, probit regression,\nPoisson regression), stochastic blockmodels of networks, hierar-\nchical mixtures of experts, and a variety of mixed-membership\nmodels (which we will discuss below).\nWorking in this family simplifies variational inference: it\nis easier to derive the corresponding CAVI algorithm, and it\nenables variational inference to scale up to massive data. In\nSection 4.1, we develop the general case. In Section 4.2, we\ndiscuss conditionally conjugate models, that is, the common\nBayesian application where some latent variables are “local” to\na data point and others, usually identified with parameters, are\n“global” to the entire dataset. Finally, in Section 4.3, we describe\nThis is not a deﬁnitive comparison between variational inference and MCMC.\nOther samplers, such as a collapsed Gibbs sampler, may perform better than\nHamiltonian Monte Carlo sampling.\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n867\nFigure . A simulation study of a two-dimensional Gaussian mixture model. The ellipses are 2σ contours of the variational approximating factors.\nstochastic variational inference (Hoffman et al. 2013), a stochas-\ntic optimization algorithm that scales up variational inference in\nthis setting.\n4.1. Complete Conditionals in the Exponential Family\nConsider the generic model p(z, x) of Section 2.1 and suppose\neach complete conditional is in the exponential family:\np(z j | z−j, x) = h(z j) exp{η j(z−j, x)⊤z j −a(η j(z−j, x))},\n(36)\nwhere zj is its own sufficient statistic, h(·) is a base measure,\nand a(·) is the log normalizer (Brown 1986). Because this is a\nconditional density, the parameter η j(z−j, x) is a function of the\nconditioning set.\nConsider mean-field variational inference for this class of\nmodels, where we fit q(z) = 	\nj q j(z j). The exponential family\nassumption simplifies the coordinate update of Equation (17),\nq(z j) ∝exp\n\nE\n\nlog p(z j | z−j, x)\n\n(37)\n= exp\n\nlog h(z j) + E\n\nη j(z−j, x)\n⊤z j\n−E\n\na(η j(z−j, x))\n \n(38)\n∝h(z j) exp\n\nE\n\nη j(z−j, x)\n⊤z j\n\n.\n(39)\nThis update reveals the parametric form of the optimal varia-\ntional factors. Each one is in the same exponential family as its\ncorresponding complete conditional. Its parameter has the same\n868\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nFigure . Red, green, and blue channel image histograms for two images from the imageCLEF dataset. The top image lacks blue hues, which is reﬂected in its blue channel\nhistogram. The bottom image has a few dominant shades of blue and green, as seen in the peaks of its histogram.\nFigure . Example clusters from the Gaussian mixture model. We assign each image to its most likely mixture cluster. The subﬁgures show nine randomly sampled images\nfrom four clusters; their namings are subjective.\ndimension and it has the same base measure h(·) and log nor-\nmalizer a(·).\nHaving established their parametric forms, let νj denote the\nvariational parameter for the jth variational factor. When we\nupdate each factor, we set its parameter equal to the expected\nparameter of the complete conditional,\nνj = E\n\nη j(z−j, x)\n\n.\n(40)\nFigure . Comparison of CAVI to a Hamiltonian Monte Carlo-based sampling tech-\nnique. CAVI ﬁts a Gaussian mixture model to ,images in less than a minute.\nThis expression facilitates deriving CAVI algorithms for many\ncomplex models.\n4.2. Conditional Conjugacy and Bayesian Models\nOne important special case of exponential family models are\nconditionally conjugate models with local and global variables.\nModels like this come up frequently in Bayesian statistics and\nstatistical machine learning, where the global variables are the\n“parameters” and the local variables are per-data-point latent\nvariables.\nConditionally conjugate models. Let β be a vector of global\nlatent variables, which potentially govern any of the data. Let z\nbe a vector of local latent variables, whose ith component only\ngoverns data in the ith “context.” The joint density is\np(β, z, x) = p(β)\nn\n\ni=1\np(zi, xi | β).\n(41)\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n869\nThe mixture of Gaussians of Section 3 is an example. The global\nvariables are the mixture components; the ith local variable is\nthe cluster assignment for data point xi.\nWe will assume that the modeling terms of Equation (41) are\nchosen to ensure each complete conditional is in the exponential\nfamily. In detail, we first assume the joint density of each (xi, zi)\npair, conditional on β, has an exponential family form,\np(zi, xi | β) = h(zi, xi) exp{β⊤t(zi, xi) −a(β)},\n(42)\nwhere t(·, ·) is the sufficient statistic.\nNext, we take the prior on the global variables to be the cor-\nresponding conjugate prior (Diaconis et al. 1979; Bernardo and\nSmith 1994),\np(β) = h(β) exp{α⊤[β, −a(β)] −a(α)}.\n(43)\nThis prior has natural (hyper)parameter α = [α1, α2]⊤, a col-\numn vector, and sufficient statistics that concatenate the global\nvariable and its log normalizer in the density of the local\nvariables.\nWith the conjugate prior, the complete conditional of the\nglobal variables is in the same family. Its natural parameter is\nˆα =\n\nα1 +\nn\n\ni=1\nt(zi, xi), α2 + n\n⊤\n.\n(44)\nTurn now to the complete conditional of the local variable zi.\nGiven β and xi, the local variable zi is conditionally independent\nof the other local variables z−i and other data x−i. This follows\nfrom the form of the joint density in Equation (41). Thus,\np(zi | xi, β, z−i, x−i) = p(zi | xi, β).\n(45)\nWe further assume that this density is in an exponential family,\np(zi | xi, β) = h(zi) exp\n\nη(β, xi)⊤zi −a(η(β, xi))\n\n. (46)\nThis is a property of the local likelihood term p(zi, xi | β) from\nEquation (42). For example, in the mixture of Gaussians, the\ncomplete conditional of the local variable is a categorical.\nVariational inference in conditionally conjugate models. We\nnow describe CAVI for this general class of models. Write\nq(β | λ) for the variational posterior approximation on β; we call\nλ the “global variational parameter.” It indexes the same expo-\nnential family density as the prior. Similarly, let the variational\nposterior q(zi | ϕi) on each local variable zi be governed by a\n“local variational parameter” ϕi. It indexes the same exponential\nfamily density as the local complete conditional. CAVI iterates\nbetween updating each local variational parameter and updating\nthe global variational parameter.\nThe local variational update is\nϕi = Eλ [η(β, xi)] .\n(47)\nThis is an application of Equation (40), where we take the expec-\ntation of the natural parameter of the complete conditional in\nEquation (45).\nThe global variational update applies the same technique.\nIt is\nλ =\n\nα1 +\nn\n\ni=1\nEϕi [t(zi, xi)] , α2 + n\n⊤\n.\n(48)\nHere, we take the expectation of the natural parameter in\nEquation (44).\nCAVI optimizes the ELBO by iterating between local updates\nof each local parameter and global updates of the global param-\neters. To assess convergence, we can compute the ELBO at each\niteration (or at some lag), up to a constant that does not depend\non the variational parameters,\nelbo =\n\nα1 +\nn\n\ni=1\nEϕi [t(zi, xi)]\n⊤\nEλ [β]\n−(α2 + n)Eλ [a(β)] −E\n\nlog q(β, z)\n\n.\n(49)\nThis is the ELBO in Equation (13) applied to the joint in Equa-\ntion (41) and the corresponding mean-field variational density;\nwe have omitted terms that do not depend on the variational\nparameters. The last term is\nE\n\nlog q(β, z)\n\n= λ⊤Eλ [t(β)]\n−a(λ) +\nn\n\ni=1\nϕ⊤\ni Eϕi [zi] −a(ϕi). (50)\nCAVI for the mixture of Gaussians model (Algorithm 2) is an\ninstance of this method. Appendix C in the online supplement\npresents another example of CAVI for latent Dirichlet allocation\n(LDA), a probabilistic topic model.\n4.3. Stochastic Variational Inference\nModern applications of probability models often require analyz-\ning massive data. However, most posterior inference algorithms\ndo not easily scale. CAVI is no exception, particularly in the con-\nditionally conjugate setting of Section 4.2. The reason is that the\ncoordinate ascent structure of the algorithm requires iterating\nthrough the entire dataset at each iteration. As the dataset size\ngrows, each iteration becomes more computationally expensive.\nAn alternative to coordinate ascent is gradient-based opti-\nmization, which climbs the ELBO by computing and follow-\ning its gradient at each iteration. This perspective is the key\nto scaling up variational inference using stochastic variational\ninference (SVI) (Hoffman et al. 2013), a method that combines\nnatural gradients (Amari 1998) and stochastic optimization\n(Robbins and Monro 1951).\nSVI focuses on optimizing the global variational parameters\nλ of a conditionally conjugate model. The flow of computation is\nsimple. The algorithm maintains a current estimate of the global\nvariational parameters. It repeatedly (a) subsamples a data point\nfrom the full dataset; (b) uses the current global parameters to\ncompute the optimal local parameters for the subsampled data\npoint; and (c) adjusts the current global parameters in an appro-\npriate way. SVI is detailed in Algorithm 3. We now show why it\nis a valid algorithm for optimizing the ELBO.\nThe natural gradient of the ELBO. In gradient-based opti-\nmization, the natural gradient accounts for the geometric\nstructure of probability parameters (Amari 1982, 1998). Specif-\nically, natural gradients warp the parameter space in a sensible\nway, so that moving the same distance in different directions\namounts to equal change in symmetrized KL divergence. The\nusual Euclidean gradient does not enjoy this property.\nIn exponential families, we find the natural gradient with\nrespect to the parameter by premultiplying the usual gradient\nby the inverse covariance of the sufficient statistic, a′′(λ)−1.\n870\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nThis is the inverse Riemannian metric and the inverse Fisher\ninformation matrix (Amari 1982).\nConditionally conjugate models enjoy simple natural gradi-\nents of the ELBO. We focus on gradients with respect to the\nglobal parameter λ. Hoffman et al. (2013) derived the Euclidean\ngradient of the ELBO,\n∇λelbo = a′′(λ)(Eϕ[ ˆα] −λ),\n(51)\nwhere Eϕ[ ˆα] is in Equation (48). Premultiplying by the inverse\nFisher information gives the natural gradient g(λ),\ng(λ) = Eϕ[ ˆα] −λ.\n(52)\nIt is the difference between the coordinate updates Eϕ[ ˆα] and\nthe variational parameters λ at which we are evaluating the gra-\ndient. In addition to enjoying good theoretical properties, the\nnatural gradient is easier to calculate than the Euclidean gradi-\nent. For more on natural gradients and variational inference, see\nSato (2001) and Honkela et al. (2008).\nWe can use this natural gradient in a gradient-based opti-\nmization algorithm. At each iteration, we update the global\nparameters,\nλt = λt−1 + ϵtg(λt),\n(53)\nwhere ϵt is a step size.\nSubstituting Equation (52) into the second term reveals a spe-\ncial structure,\nλt = (1 −ϵt)λt−1 + ϵtEϕ[ ˆα].\n(54)\nNotice this does not require additional types of calculations\nother than those for coordinate ascent updates. At each itera-\ntion, we first compute the coordinate update. We then adjust the\ncurrent estimate to be a weighted combination of the update and\nthe current variational parameter.\nThough easy to compute, using the natural gradient has the\nsame cost as the coordinate update in Equation (48); it requires\nsumming over the entire dataset and computing the optimal\nlocal variational parameters for each data point. With massive\ndata, this is prohibitively expensive.\nStochastic optimization of the ELBO. Stochastic variational\ninference solves this problem by using the natural gradient in\na stochastic optimization algorithm. Stochastic optimization\nalgorithms follow noisy but cheap-to-compute gradients to\nreach the optimum of an objective function. (In the case of the\nELBO, stochastic optimization will reach a local optimum.)\nIn their seminal article, Robbins and Monro (1951) proved\nresults implying that optimization algorithms can successfully\nuse noisy, unbiased gradients, as long as the step size sequence\nsatisfies certain conditions. This idea has blossomed (Kushner\nand Yin 1997; Spall 2003). Stochastic optimization has enabled\nmodern machine learning to scale to massive data (Le Cun and\nBottou 2004).\nOur aim is to construct a cheaply computed, noisy, unbiased\nnatural gradient. We expand the natural gradient in Equation\n(52) using Equation (44):\ng(λ) = α +\n n\n\ni=1\nEϕ∗\ni [t(zi, xi)] , n\n⊤\n−λ,\n(55)\nwhere ϕ∗\ni indicates that we consider the optimized local vari-\national parameters (at fixed global parameters λ) in Equation\n(47). We construct a noisy natural gradient by sampling an index\nfrom the data and then rescaling the second term,\nt ∼Unif(1, . . . , n)\n(56)\nˆg(λ) = α + n\n\nEϕ∗\nt [t(zt, xt)] , 1\n⊤−λ.\n(57)\nThe noisy natural gradient ˆg(λ) is unbiased: Et\nˆg(λ)\n\n= g(λ).\nAnd it is cheap to compute—it only involves a single sampled\ndata point and only one set of optimized local parameters. (This\nimmediately extends to minibatches, where we sample B data\npoints and rescale appropriately.) Again, the noisy gradient only\nrequires calculations from the coordinate ascent algorithm. The\nfirst two terms of Equation (57) are equivalent to the coordinate\nupdate in a model with n replicates of the sampled data point.\nFinally, we set the step size sequence. It must follow the con-\nditions by Robbins and Monro (1951),\n\nt\nϵt = ∞\n;\n\nt\nϵ2\nt < ∞.\n(58)\nMany sequences will satisfy these conditions, for example, ϵt =\nt−κ for κ ∈(0.5, 1]. The full SVI algorithm is in Algorithm 3.\nWe emphasize that SVI requires no new derivation beyond\nwhat is needed for CAVI. Any implementation of CAVI can be\nimmediately scaled up to a stochastic algorithm.\nProbabilistic topic models. We demonstrate SVI with a\nprobabilistic topic model. Probabilistic topic models are mixed-\nmembership models of text, used to uncover the latent “topics”\nthat run through a collection of documents. Topic models have\nbecome a popular technique for exploratory data analysis of\nlarge collections (Blei 2012).\nIn detail, each latent topic is a distribution over terms in\na vocabulary and each document is a collection of words that\ncomes from a mixture of the topics. The topics are shared\nacross the collection, but each document mixes them with differ-\nent proportions. (This is the hallmark of a mixed-membership\nmodel.) Thus, topic modeling casts topic discovery as a posterior\ninference problem. Posterior estimates of the topics and topic\nproportions can be used to summarize, visualize, explore, and\nform predictions about the documents.\nOne motivation for topic modeling is to get a handle on mas-\nsive collections of documents. Early inference algorithms were\nbased on coordinate ascent variational inference (Blei, Ng, and\nJordan 2003) and analyzed collections in the thousands or tens\nof thousands of documents. (Appendix C presents this algo-\nrithm). With SVI, topic models scale up to millions of docu-\nments; the details of the algorithm are in Hoffman et al. (2013).\nFigure 7 illustrates topics inferred using the latent Dirichlet allo-\ncation model (Blei, Ng, and Jordan 2003) from 1.8M articles\nfrom the New York Times. This analysis would not have been\npossible without SVI.\n5. Discussion\nWe described variational inference, a method that uses opti-\nmization to make probabilistic computations. The goal is to\napproximate the conditional density of latent variables z given\nobserved variables x, p(z | x). The idea is to posit a family of\ndensities Q and then to find the member q∗(·) that is closest\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n871\nAlgorithm 3: SVI for conditionally conjugate models\nInput: Model p(x, z), data x, and step size sequence ϵt\nOutput: Global variational densities qλ(β)\nInitialize: Variational parameters λ0\nwhile TRUE do\nChoose a data point uniformly at random,\nt ∼Unif(1, . . . , n)\nOptimize its local variational parameters\nϕ∗\nt = Eλ [η(β, xt)]\nCompute the coordinate update as though xt were\nrepeated n times,\nˆλ = α + nE[ϕ∗\nt f (zt, xt)]\nUpdate the global variational parameter,\nλt = (1 −ϵt)λt + ϵtˆλt\nend\nreturn λ\nin KL divergence to the conditional of interest. Minimizing the\nKL divergence is the optimization problem, and its complexity\nis governed by the complexity of the approximating family.\nWe then described the mean-field family, that is, the family of\nfully factorized densities of the latent variables. Using this fam-\nily, variational inference is particularly amenable to coordinate-\nascent optimization, which iteratively optimizes each factor.\nThis approach closely connects to the classical Gibbs sampler\n(Geman and Geman 1984; Gelfand and Smith 1990). We showed\nhow to use mean-field VI to approximate the posterior density\nof a Bayesian mixture of Gaussians, discussed the special case of\nexponential families and conditional conjugacy, and described\nthe extension to stochastic variational inference (Hoffman et al.\n2013), which scales mean-field variational inference to massive\ndata.\nFigure . Topics found in a corpus of .M articles from the New York Times. Repro-\nduced with permission from Hoﬀman et al. ().\n5.1. Applications\nResearchers in many fields have used variational inference to\nsolve real problems. Here, we focus on example applications\nof mean-field variational inference and structured variational\ninference based on the KL divergence. This discussion is not\nexhaustive; our intention is to outline the diversity of applica-\ntions of variational inference.\nComputational biology. VI is widely used in computational\nbiology, where probabilistic models provide important building\nblocks for analyzing genetic data. For example, VI has been used\nin genome-wide association studies (Logsdon, Hoffman, and\nMezey 2010; Carbonetto and Stephens 2012), regulatory net-\nwork analysis (Sanguinetti, Lawrence, and Rattray 2006), motif\ndetection (Xing et al. 2004), phylogenetic hidden Markov mod-\nels (Jojic et al. 2004), population genetics (Raj, Stephens, and\nPritchard 2014), and gene expression analysis (Stegle et al. 2010).\nComputer vision and robotics. Since its inception, varia-\ntional inference has been important to computer vision. Vision\nresearchers frequently analyze large and high-dimensional\ndatasets of images, and fast inference is important to successfully\ndeploy a vision system. Some of the earliest examples included\ninferring nonlinear image manifolds (Bishop and Winn 2000)\nand finding layers of images in videos (Jojic and Frey 2001).\nAs other examples, variational inference is important to prob-\nabilistic models of videos (Chan and Vasconcelos 2009; Wang\nand Mori 2009), image denoising (Likas and Galatsanos 2004),\ntracking (Vermaak, Lawrence, and Pérez 2003; Yu and Wu\n2005), place recognition and mapping for robotics (Cummins\nand Newman 2008; Ramos et al. 2012), and image segmentation\nwith Bayesian nonparametrics (Sudderth and Jordan 2009). Du\net al. (2009) used variational inference in a probabilistic model to\ncombine the tasks of segmentation, clustering, and annotation.\nComputational neuroscience. Modern neuroscience research\nalso requires analyzing very large and high-dimensional\ndatasets, such as high-frequency time series data or high-\nresolution functional magnetic imaging data. There have been\nmany applications of variational inference to neuroscience,\nespecially for autoregressive processes (Roberts and Penny 2002;\nPenny, Kiebel, and Friston 2003; Penny, Trujillo-Barreto, and\nFriston 2005; Flandin and Penny 2007; Harrison and Green\n2010). Other applications of variational inference to neuro-\nscience include hierarchical models of multiple subjects (Wool-\nrich et al. 2004), spatial models (Sato et al. 2004; Zumer et al.\n2007; Kiebel et al. 2008; Wipf and Nagarajan 2009; Lashkari et al.\n2012; Nathoo et al. 2014), brain-computer interfaces (Sykacek,\nRoberts, and Stokes 2004), and factor models (Manning et al.\n2014; Gershman et al. 2014). There is a software toolbox that\nuses variational methods for solving neuroscience and psychol-\nogy research problems (Daunizeau et al. 2014).\nNatural language processing and speech recognition. In natu-\nral language processing, variational inference has been used for\nsolving problems such as parsing (Liang et al. 2007; Liang, Jor-\ndan, and Klein 2009), grammar induction (Kurihara and Sato\n2006; Naseem et al. 2010; Cohen and Smith 2010), models of\nstreaming text (Yogatama et al. 2014), topic modeling (Blei,\nNg, and Jordan 2003), and hidden Markov models and part-of-\nspeech tagging (Wang and Blunsom 2013). In speech recogni-\ntion, variational inference has been used to fit complex coupled\n872\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nhidden Markov models (Reyes-Gomez, Ellis, and Jojic 2004) and\nswitching dynamic systems (Deng 2004).\nOther applications. There have been many other applications\nof variational inference. Fields in which it has been used include\neconomics (Braun and McAuliffe 2010), optimal control and\nreinforcement learning (Van Den Broek, Wiegerinck, and Kap-\npen 2008; Furmston and Barber 2010), statistical network anal-\nysis (Wiggins and Hofman 2008; Airoldi et al. 2008), astronomy\n(Regier et al. 2015), and the social sciences (Erosheva, Fienberg,\nand Joutard 2007; Grimmer 2011). General variational inference\nalgorithms have been developed for a variety of classes of mod-\nels, including shrinkage models (Armagan, Clyde, and Dunson\n2011; Armagan and Dunson 2011; Neville, Ormerod, and Wand\n2014), general time-series models (Roberts et al. 2004; Barber\nand Chiappa 2006; Archambeau et al. 2007a, 2007b; Johnson\nand Willsky 2014; Foti et al. 2014), robust models (Tipping and\nLawrence 2005; Wang and Blei 2015), and Gaussian process\nmodels (Titsias and Lawrence 2010; Damianou, Titsias, and\nLawrence 2011; Hensman, Fusi, and Lawrence 2013).\n5.2. Theory\nThough researchers have not developed much theory around\nvariational inference, there are several threads of research about\ntheoretical guarantees of variational approximations. As we\nmentioned in the introduction, one of our purposes for writing\nthis article is to catalyze research on the statistical theory around\nvariational inference.\nBelow, we summarize a variety of results. In general, they\nare all of the following type: treat VI posterior means as point\nestimates (or use m-step estimates from variational EM) and\nconfirm that they have the usual frequentist asymptotics.\n(Sometimes the research finds that they do not enjoy the same\nasymptotics.) Each result revolves around a single model and a\nsingle family of variational approximations.\nYou, Ormerod, and Muller (2014) studied the variational\nposterior for a classical Bayesian linear model. They put a nor-\nmal prior on the coefficients and an inverse gamma prior on\nthe response variance. They found that, under standard regu-\nlarity conditions, the mean-field variational posterior mean of\nthe parameters is consistent in the frequentist sense. Ormerod,\nYou, and Muller (2014) built on their earlier work with a spike-\nand-slab prior on the coefficients and found similar consistency\nresults.\nHall, Ormerod, and Wand (2011a) and Hall et al. (2011b)\nexamined a simple Poisson mixed-effects model, one with a sin-\ngle predictor and a random intercept. They used a Gaussian\nvariational approximation and estimated parameters with vari-\national EM. They proved consistency of these estimates at the\nparametric rate and showed asymptotic normality with asymp-\ntotically valid standard errors.\nCelisse et al. (2012) and Bickel et al. (2013) analyzed network\ndata using stochastic blockmodels. They showed asymptotic\nnormality of parameter estimates obtained using a mean-field\nvariational approximation. They highlighted the computa-\ntional advantages and theoretical guarantees of the variational\napproach over maximum likelihood for dense, sparse, and\nrestricted variants of the stochastic blockmodel.\nWestling and McCormick (2015) studied the consistency of\nVI through a connection to M-estimation. They focused on a\nbroader class of models (with posterior support in real coordi-\nnate space) and analyzed an automated VI technique that uses\na Gaussian variational approximation (Kucukelbir et al. 2015).\nThey derived an asymptotic covariance matrix estimator of the\nvariational approximation and showed its robustness to model\nmisspecification.\nFinally, Wang and Titterington (2006) analyzed variational\napproximations to mixtures of Gaussians. Specifically, they con-\nsidered Bayesian mixtures with conjugate priors, the mean-field\nvariational approximation, and an estimator that is the varia-\ntional posterior mean. They confirmed that CAVI converges\nto a local optimum, that the VI estimator is consistent, and\nthat the VI estimate and maximum likelihood estimate (MLE)\napproach each other at a rate of O(1/n). Wang and Tittering-\nton (2005), showed that the asymptotic variational posterior\ncovariance matrix is “too small”—it differs from the MLE covari-\nance (i.e., the inverse Fisher information) by a positive-definite\nmatrix.\n5.3. Beyond Conditional Conjugacy\nWe focused on models where the complete conditional is in the\nexponential family. Many models, however, do not enjoy this\nproperty. A simple example is Bayesian logistic regression,\nβk ∼N (0, 1),\nyi | xi, β ∼Bern(σ (β⊤xi)),\nwhere σ (·) is the logistic function. The posterior density of\nthe coefficients is not in an exponential family and we cannot\napply the variational inference methods we discussed above.\nSpecifically, we cannot compute the expectations in the first\nterm of the ELBO in Equation (13) or the coordinate update in\nEquation (18).\nExploring variational methods for such models has been a\nfruitful area of research. An early example is Jaakkola and Jor-\ndan (1997, 2000), who developed a variational bound tailored\nto logistic regression. Blei and Lafferty (2007) later adapted\ntheir idea to nonconjugate topic models, and researchers have\ncontinued to improve the original bound (Khan et al. 2010;\nMarlin, Khan, and Murphy 2011; Ermis and Bouchard 2014). In\nother work, Braun and McAuliffe (2010) derived a variational\ninference algorithm for the discrete choice model, which also\nlies outside of the class of conditionally conjugate models.\nThey developed a delta method to approximate the difficult-to-\ncompute expectations. Finally, Wand et al. (2011) used auxiliary\nvariable methods, quadrature, and mixture approximations\nto handle a variety of likelihood terms that fall outside of the\nexponential family.\nMore recently, researchers have generalized nonconjugate\ninference, seeking recipes that can be used across many models.\nWang and Blei (2013) adapted Laplace approximations and the\ndelta method to this end, improving inference in nonconjugate\ngeneralized linear models and topic models; this approach is\nalso used by Bugbee, Breidt, and van der Woerd (2016) for\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n873\nsemiparametric regression. Knowles and Minka (2011) gen-\neralized the Jaakkola and Jordan (1997, 2000) bound in a\nmessage-passing algorithm and Wand (2014) further simplified\nand extended their approach. Tan and Nott (2013, 2014) applied\nthese message-passing methods to generalized linear mixed\nmodels (and also combined them with SVI). Rohde and Wand\n(2016) unified many of these algorithmic developments and pro-\nvided practical insights into their numerical implementations.\nFinally, there has been a flurry of research on optimizing\ndifficult variational objectives with Monte Carlo (MC) esti-\nmates of the gradient. The idea is to write the gradient of the\nELBO as an expectation, compute MC estimates of it, and then\nuse stochastic optimization with repeated MC gradients. This\nfirst appeared independently in several articles (Ji, Shen, and\nWest 2010; Nott et al. 2012; Paisley, Blei, and Jordan 2012;\nWingate and Weber 2013). The newest approaches avoid any\nmodel-specific derivations, and are termed “black box” infer-\nence methods. As examples, see Kingma and Welling (2014);\nRezende, Mohamed, and Wierstra (2014); Ranganath, Gerrish,\nand Blei (2014); Ranganath, Tran, and Blei (2016); Salimans and\nKnowles (2014); Titsias and Lázaro-Gredilla (2014); and Tran,\nRanganath, and Blei (2016). Kucukelbir et al. (2017) leveraged\nthese ideas toward an automatic VI technique that works on\nany model written in the probabilistic programming system\nStan (Stan Development Team 2015). This is a step toward a\nderivation-free, easy-to-use VI algorithm.\n5.4. Open Problems\nThere are many open avenues for statistical research in varia-\ntional inference.\nWe focused on optimizing kl\n\nq(z)||p(z | x)\n\nas the vari-\national objective function. A promising avenue of research is\nto develop variational inference methods that optimize other\nmeasures, such as α-divergence measures. As one example,\nexpectation propagation (Minka 2001) is inspired by the KL\ndivergence “in the other direction,” between p(z | x) and q(z).\nOther work has developed divergences based on lower bounds\nthat are tighter than the ELBO (Barber and de van Laar 1999;\nLeisink and Kappen 2001). While alternative divergences may\nbe difficult to optimize, they may give better approximations\n(Minka 2005; Opper and Winther 2005).\nThough it is flexible, the mean-field family makes strong\nindependence assumptions. These assumptions help with scal-\nable optimization, but they limit the expressibility of the vari-\national family. Further, they can exacerbate issues with local\noptima of the objective and underestimating posterior vari-\nances; see Figure 1. A second avenue of research is to develop\nbetter approximations while maintaining efficient optimization.\nAs we mentioned previously, structured variational inference\nhas its roots in the early days of the method (Saul and Jordan\n1996; Barber and Wiegerinck 1999). More recently, Hoffman\nand Blei (2015) used generic structured variational inference\nin a stochastic optimization algorithm; Kucukelbir et al. (2017),\nChallis and Barber (2013), and Tan and Nott (2017) took advan-\ntage of Gaussian variational families with nondiagonal covari-\nance; Giordano, Broderick, and Jordan (2015) post-processed\nthe mean-field parameters to correct for underestimating the\nvariance; and Ranganath, Tran, and Blei (2016) embedded the\nmean-field parameters themselves in a hierarchical model to\ninduce variational dependencies between latent variables.\nThe interface between variational inference and MCMC\nremains relatively unexplored. de Freitas et al. (2001) used fit-\nted variational distributions as a component of a proposal dis-\ntribution for Metropolis–Hastings. Hoffman, Blei, and Mimno\n(2012) and Hoffman and Blei (2015) studied MCMC as a\nmethod of approximating coordinate updates, for example, to\ninclude structure in the variational family. Salimans, Kingma,\nand Welling (2015) proposed a variational approximation to\nthe MCMC chain; their method enables an explicit trade off\nbetween computational accuracy and speed. Understanding\nhow to combine these two strategies for approximate inference\nis a ripe area for future research. A principled analysis of when\nto use (and combine) variational inference and MCMC would\nhave both theoretical and practical impact in the field.\nFinally, the statistical properties of variational inference are\nnot yet well understood, especially in contrast to the wealth of\nanalysis of MCMC techniques. There has been some progress;\nsee Section 5.2. A final open research problem is to understand\nvariational inference as an estimator and to understand its sta-\ntistical profile relative to the exact posterior.\nSupplementary Materials\nThe online supplementary materials contain the appendices for the article.\nReferences\nAhmed, A., Aly, M., Gonzalez, J., Narayanamurthy, S., and Smola, A. (2012),\n“Scalable Inference in Latent Variable Models,” in International Confer-\nence on Web Search and Data Mining, pp. 123–132. [860]\nAiroldi, E., Blei, D., Fienberg, S., and Xing, E. (2008), “Mixed Member-\nship Stochastic Blockmodels,” Journal of Machine Learning Research,\n9, 1981–2014. [872]\nAmari, S. (1982), “Differential Geometry of Curved Exponential Families-\nCurvatures and Information Loss,” The Annals of Statistics, 10, 357–\n385. [869]\n——— (1998), “Natural Gradient Works Efficiently in Learning,” Neural\nComputation, 10, 251–276. [869]\nArchambeau, C., Cornford, D., Opper, M., and Shawe-Taylor, J. (2007a),\n“Gaussian Process Approximations of Stochastic Differential Equa-\ntions,” Workshop on Gaussian Processes in Practice, 1, 1–16. [872]\nArchambeau, C., Opper, M., Shen, Y., Cornford, D., and Shawe-Taylor,\nJ. (2007b), “Variational Inference for Diffusion Processes,” in Neural\nInformation Processing Systems, pp. 17–24. [872]\nArmagan, A., Clyde, M., and Dunson, D. (2011), “Generalized Beta Mix-\ntures of Gaussians,” in Neural Information Processing Systems, pp. 523–\n531. [872]\nArmagan, A., and Dunson, D. (2011), “Sparse Variational Analysis of Linear\nMixed Models for Large Data Sets,” Statistics & Probability Letters, 81,\n1056–1062. [872]\nBarber, D. (2012), Bayesian Reasoning and Machine Learning, Cambridge,\nUK: Cambridge University Press. [860]\nBarber, D., and Bishop, C. M. (1998), “Ensemble Learning in Bayesian\nNeural Networks,” in Generalization in Neural Networks and\nMachine Learning, ed. C. M. Bishop, New York: Springer Verlag,\npp. 215–237. [860]\nBarber, D., and Chiappa, S. (2006), “Unified Inference for Variational\nBayesian Linear Gaussian State-Space Models,” in Neural Information\nProcessing Systems, pp. 81–88. [872]\nBarber, D., and de van Laar, P. (1999), “Variational Cumulant Expansions\nfor Intractable Distributions,” Journal of Artificial Intelligence Research,\n10, 435–455. [873]\n874\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nBarber, D., and Wiegerinck, W. (1999), “Tractable Variational Structures\nfor Approximating Graphical Models,” in Neural Information Processing\nSystems, pp. 183–189. [863,873]\nBeal, M., and Ghahramani, Z. (2003), “The Variational Bayesian EM Algo-\nrithm for Incomplete Data: With Application to Scoring Graphical\nModel Structures,” in Bayesian Statistics (Vol. 7), eds. J. Bernardo, M.\nBayarri, J. Berger, A. Dawid, D. Heckerman, A. Smith, and M. West,\nOxford, UK: Oxford University Press, pp. 453–464. [862]\nBernardo, J., and Smith, A. (1994), Bayesian Theory, Chichester, UK:\nWiley. [869]\nBickel, P., Choi, D., Chang, X., and Zhang, H. (2013), “Asymptotic Nor-\nmality of Maximum Likelihood and its Variational Approximation for\nStochastic Blockmodels,” The Annals of Statistics, 41, 1922–1943. [872]\nBishop, C. (2006), Pattern Recognition and Machine Learning, New York:\nSpringer. [863,864]\nBishop, C., Lawrence, N., Jaakkola, T., and Jordan, M. I. (1998), “Approxi-\nmating Posterior Distributions in Belief Networks using Mixtures,” in\nNeural Information Processing Systems, pp. 416–422. [863]\nBishop, C., and Winn, J. (2000), “Non-linear Bayesian Image Modelling,” in\nEuropean Conference on Computer Vision, pp. 3–17. [871]\nBlei, D. (2012), “Probabilistic Topic Models,” Communications of the ACM,\n55, 77–84. [870]\nBlei, D., and Jordan, M. I. (2006), “Variational Inference for Dirichlet Pro-\ncess Mixtures,” Journal of Bayesian Analysis, 1, 121–144. [860]\nBlei, D., and Lafferty, J. (2007), “A Correlated Topic Model of Science,”\nAnnals of Applied Statistics, 1, 17–35. [872]\nBlei, D., Ng, A., and Jordan, M. I. (2003), “Latent Dirichlet Allocation,” Jour-\nnal of Machine Learning Research, 3, 993–1022. [870,871]\nBraun, M., and McAuliffe, J. (2010), “Variational Inference for Large-Scale\nModels of Discrete Choice,” Journal of the American Statistical Associ-\nation, 105, 324–335. [860,872]\nBrown, L. (1986), Fundamentals of Statistical Exponential Families, Hay-\nward, CA: Institute of Mathematical Statistics. [867]\nBugbee, B., Breidt, F., and van der Woerd, M. (2016), “Laplace Variational\nApproximation for Semiparametric Regression in the Presence of Het-\neroscedastic Errors,” Journal of Computational and Graphical Statistics,\n25, 225–245. [872]\nCarbonetto, P., and Stephens, M. (2012), “Scalable Variational Inference for\nBayesian Variable Selection in Regression, and its Accuracy in Genetic\nAssociation Studies,” Bayesian Analysis, 7, 73–108. [871]\nCelisse, A., Daudin, J.-J., and Pierre, L. (2012), “Consistency of Maximum-\nLikelihood and Variational Estimators in the Stochastic Block Model,”\nElectronic Journal of Statistics, 6, 1847–1899. [872]\nChallis, E., and Barber, D. (2013), “Gaussian Kullback-Leibler Approx-\nimate Inference,” The Journal of Machine Learning Research, 14,\n2239–2286. [873]\nChan, A., and Vasconcelos, N. (2009), “Layered Dynamic Textures,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 31, 1862–\n1879. [871]\nCohen, S., and Smith, N. (2010), “Covariance in Unsupervised Learning\nof Probabilistic Grammars,” The Journal of Machine Learning Research,\n11, 3017–3051. [871]\nCummins, M., and Newman, P. (2008), “FAB-MAP: Probabilistic Localiza-\ntion and Mapping in the Space of Appearance,” The International Jour-\nnal of Robotics Research, 27, 647–665. [871]\nde Freitas, N. D., Højen-Sørensen, P., Jordan, M., and Russell, S. (2001),\n“Variational MCMC,” in Uncertainty in Artificial Intelligence, pp. 120–\n127. [873]\nDamianou, A., Titsias, M., and Lawrence, N. (2011), “Variational Gaussian\nProcess Dynamical Systems,” in Neural Information Processing Systems,\npp. 2510–2518. [872]\nDaunizeau, J., Adam, V., and Rigoux, L. (2014), “VBA: A Probabilistic Treat-\nment of Nonlinear Models for Neurobiological and Behavioural Data,”\nPLoS Computational Biology, 10, e1003441. [871]\nDempster, A., Laird, N., and Rubin, D. (1977), “Maximum Likelihood from\nIncomplete Data via the EM Algorithm,” Journal of the Royal Statistical\nSociety, Series B, 39, 1–38. [860,862]\nDeng, L. (2004), “Switching Dynamic System Models for Speech Articula-\ntion and Acoustics,” in Mathematical Foundations of Speech and Lan-\nguage Processing, eds. M. Johnson, S. P. Khudanpur, M. Ostendorf, and\nR. Rosenfeld, New York: Springer, pp. 115–133. [872]\nDiaconis, P., and Ylvisaker, D. (1979), “Conjugate Priors for Exponential\nFamilies,” The Annals of Statistics, 7, 269–281. [869]\nDu, L., Lu, R., Carin, L., and Dunson, D. (2009), “A Bayesian Model for\nSimultaneous Image Clustering, Annotation and Object Segmenta-\ntion,” in Neural Information Processing Systems, pp. 486–494. [871]\nErmis, B., and Bouchard, G. (2014), “Iterative Splits of Quadratic Bounds\nfor Scalable Binary Tensor Factorization,” in Uncertainty in Artificial\nIntelligence, pp. 192–199. [872]\nErosheva, E. A., Fienberg, S. E., and Joutard, C. (2007), “Describing Disabil-\nity through Individual-Level Mixture Models for Multivariate Binary\nData,” The Annals of Applied Statistics, 1, 346–384. [872]\nFlandin, G., and Penny, W. (2007), “Bayesian fMRI Data Analysis with\nSparse Spatial Basis Function Priors,” NeuroImage, 34, 1108–1125.\n[871]\nFoti, N., Xu, J., Laird, D., and Fox, E. (2014), “Stochastic Variational Infer-\nence for Hidden Markov Models,” in Neural Information Processing Sys-\ntems, pp. 3599–3607. [872]\nFurmston, T., and Barber, D. (2010), “Variational Methods for Rein-\nforcement Learning,” Artificial Intelligence and Statistics, 9, 241–248.\n[872]\nGelfand, A., and Smith, A. (1990), “Sampling Based Approaches to Calcu-\nlating Marginal Densities,” Journal of the American Statistical Associa-\ntion, 85, 398–409. [859,861,864,871]\nGeman, S., and Geman, D. (1984), “Stochastic Relaxation, Gibbs Distribu-\ntions and the Bayesian Restoration of Images,” IEEE Transactions on\nPattern Analysis and Machine Intelligence, 6, 721–741. [859,864,871]\nGershman, S. J., Blei, D. M., Norman, K. A., and Sederberg, P. B. (2014),\n“Decomposing Spatiotemporal Brain Patterns into Topographic Latent\nSources,” NeuroImage, 98, 91–102. [871]\nGhahramani, Z., and Jordan, M. I. (1997), “Factorial Hidden Markov Mod-\nels,” Machine Learning, 29, 245–273. [860]\nGiordano, R. J., Broderick, T., and Jordan, M. I. (2015), “Linear Response\nMethods for Accurate Covariance Estimates from Mean Field Varia-\ntional Bayes,” in Neural Information Processing Systems, pp. 1441–1449.\n[860,873]\nGrimmer, J. (2011), “An introduction to Bayesian Inference via Variational\nApproximations,” Political Analysis, 19, 32–47. [872]\nHall, P., Ormerod, J., and Wand, M. (2011a), “Theory of Gaussian Varia-\ntional Approximation for a Poisson Mixed Model,” Statistica Sinica, 21,\n369–389. [872]\nHall, P., Pham, T., Wand, M., and Wang, S. (2011b), “Asymptotic Normality\nand Valid Inference for Gaussian Variational Approximation,” Annals\nof Statistics, 39, 2502–2532. [872]\nHarrison, L., and Green, G. (2010), “A Bayesian Spatiotemporal Model for\nvery Large Data Sets,” Neuroimage, 50, 1126–1141. [871]\nHastings, W. (1970), “Monte Carlo Sampling Methods using Markov Chains\nand their Applications,” Biometrika, 57, 97–109. [859]\nHensman, J., Fusi, N., and Lawrence, N. (2013), “Gaussian Processes for Big\nData,” in Proceedings of the Twenty-Ninth Conference on Uncertainty in\nArtificial Intelligence, Corvallis, OR: AUAI Press, pp. 282–290. [872]\nHensman, J., Rattray, M., and Lawrence, N. (2012), “Fast Variational Infer-\nence in the Conjugate Exponential Family,” in Neural Information Pro-\ncessing Systems, pp. 2888–2896. [864]\nHinton, G., and Van Camp, D. (1993), “Keeping the Neural Networks Sim-\nple by Minimizing the Description Length of the Weights,” in Compu-\ntational Learning Theory, pp. 5–13. [860]\nHoffman, M., Blei, D., and Mimno, D. M. (2012), “Sparse Stochastic Infer-\nence for Latent Dirichlet Allocation,” in Proceedings of the 29th Inter-\nnational Conference on Machine Learning (ICML-12), eds. J. Langford\nand J. Pineau, New York: ACM, pp. 1599–1606. [873]\nHoffman, M. D., Blei, D., Wang, C., and Paisley, J. (2013), “Stochastic Varia-\ntional Inference,” Journal of Machine Learning Research, 14, 1303–1347.\n[861,867,869,870,871]\nHoffman, M. D., and Blei, D. M. (2015), “Structured Stochastic Variational\nInference,” in Proceedings of the Eighteenth International Conference on\nArtificial Intelligence and Statistics (Vol. 38), eds. G. Lebanon and S. V.\nN. Vishwanathan, San Diego, CA: Proceedings of Machine Learning\nResearch, pp. 361–369. [873]\nHoffman, M. D., and Gelman, A. (2014), “The No-U-turn Sampler: Adap-\ntively Setting Path Lengths in Hamiltonian Monte Carlo,” The Journal\nof Machine Learning Research, 15, 1593–1623. [866]\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n875\nHonkela, A., Tornio, M., Raiko, T., and Karhunen, J. (2008), “Natural Con-\njugate Gradient in Variational Inference,” in Neural Information Pro-\ncessing, eds. J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, New York:\nSpringer, pp. 305–314. [870]\nJaakkola, T., and Jordan, M. I. (1996), “Computing Upper and Lower\nBounds on Likelihoods in Intractable Networks,” in Uncertainty in Arti-\nficial Intelligence, pp. 340–348. [860]\n——— (1997), “A Variational Approach to Bayesian Logistic Regression\nModels and their Extensions,” in Artificial Intelligence and Statistics,\npp. 1–12. [860,872]\n——— (2000), “Bayesian Parameter Estimation via Variational Methods,”\nStatistics and Computing, 10, 25–37. [872]\nJi, C., Shen, H., and West, M. (2010), “Bounded Approximations for\nMarginal Likelihoods,” Technical Report, Duke University. [873]\nJohnson, M., and Willsky, A. (2014), “Stochastic Variational Inference for\nBayesian Time Series Models,” in International Conference on Machine\nLearning, pp. 1854–1862. [872]\nJojic, N., and Frey, B. (2001), “Learning Flexible Sprites in Video Layers,” in\nComputer Vision and Pattern Recognition, pp. 1–8. [871]\nJojic, V., Jojic, N., Meek, C., Geiger, D., Siepel, A., Haussler, D., and\nHeckerman, D. (2004), “Efficient Approximations for Learning Phy-\nlogenetic HMM Models from Data,” Bioinformatics, 20, 161–168.\n[871]\nJordan, M. I., Ghahramani, Z., Jaakkola, T., and Saul, L. (1999), “Introduc-\ntion to Variational Methods for Graphical Models,” Machine Learning,\n37, 183–233. [859,860,862]\nKhan, M. E., Bouchard, G., Murphy, K. P., and Marlin, B. M. (2010), “Varia-\ntional Bounds for Mixed-Data Factor Analysis,” in Neural Information\nProcessing Systems, pp. 1108–1116. [872]\nKiebel, S., Daunizeau, J., Phillips, C., and Friston, K. (2008), “Varia-\ntional Bayesian Inversion of the Equivalent Current Dipole Model in\nEEG/MEG,” NeuroImage, 39, 728–741. [871]\nKingma, D., and Welling, M. (2014), “Auto-Encoding Variational Bayes,” in\nProceedings of the 2nd International Conference on Learning Represen-\ntations (ICLR). [873]\nKnowles, D., and Minka, T. (2011), “Non-Conjugate Variational Message\nPassing for Multinomial and Binary Regression,” in Neural Information\nProcessing Systems, pp. 1701–1709. [864,873]\nKucukelbir, A., Ranganath, R., Gelman, A., and Blei, D. (2015), “Automatic\nVariational Inference in Stan,” in Neural Information Processing Sys-\ntems, pp. 568–576. [860,872]\nKucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and Blei, D. M. (2017),\n“Automatic Differentiation Variational Inference,” Journal of Machine\nLearning Research, 18, 1–45. [860,873]\nKullback, S., and Leibler, R. (1951), “On Information and Sufficiency,” The\nAnnals of Mathematical Statistics, 22, 79–86. [862]\nKurihara, K., and Sato, T. (2006), “Variational Bayesian Grammar Induc-\ntion for Natural Language,” in Grammatical Inference: Algorithms and\nApplications, New York: Springer, pp. 84–96. [871]\nKushner, H., and Yin, G. (1997), Stochastic Approximation Algorithms and\nApplications, eds. Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino, and\nE. Tomita, New York: Springer. [860,870]\nLashkari, D.,\nSridharan, R., Vul, E., Hsieh, P., Kanwisher, N., and\nGolland, P. (2012), “Search for Patterns of Functional Specificity in the\nBrain: A Nonparametric Hierarchical Bayesian Model for Group fMRI\nData,” Neuroimage, 59, 1348–1368. [871]\nLauritzen, S., and Spiegelhalter, D. (1988), “Local Computations with Prob-\nabilities on Graphical Structures and their Application to Expert Sys-\ntems,” Journal of the Royal Statistical Society, Series B, 50, 157–224.\n[864]\nLe Cun, Y., and Bottou, L. (2004), “Large Scale Online Learning,” in Neural\nInformation Processing Systems, pp. 217–224. [870]\nLeisink, M., and Kappen, H. (2001), “A Tighter Bound for Graphical Mod-\nels,” Neural Computation, 13, 2149–2171. [873]\nLiang, P., Jordan, M. I., and Klein, D. (2009), “Probabilistic Grammars and\nHierarchical Dirichlet Processes,” in The Handbook of Applied Bayesian\nAnalysis, eds. T. O’Hagan, and M. West, New York: Oxford University\nPress, pp. 776–819. [871]\nLiang, P., Petrov, S., Klein, D., and Jordan, M. I. (2007), “The Infinite PCFG\nusing Hierarchical Dirichlet Processes,” in Empirical Methods in Natu-\nral Language Processing, pp. 688–697. [871]\nLikas, A., and Galatsanos, N. (2004), “A Variational Approach for Bayesian\nBlind Image Deconvolution,” IEEE Transactions on Signal Processing,\n52, 2222–2233. [871]\nLogsdon, B., Hoffman, G., and Mezey, J. (2010), “A Variational Bayes Algo-\nrithm for Fast and Accurate Multiple Locus Genome-Wide Association\nAnalysis,” BMC Bioinformatics, 11, 58. [871]\nMacKay, D. J. (1997), “Ensemble Learning for Hidden Markov Mod-\nels,”\nunpublished\nmanuscript,\navailable\nat\nhttp://www.inference.\neng.cam.ac.uk/mackay/ensemblePaper.pdf. [860]\nManning, J. R., Ranganath, R., Norman, K. A., and Blei, D. M. (2014),\n“Topographic Factor Analysis: A Bayesian Model for Inferring Brain\nNetworks from Neural Data,” PloS one, 9, e94914. [871]\nMarlin, B. M., Khan, M. E., and Murphy, K. P. (2011), “Piecewise Bounds\nfor Estimating Bernoulli-Logistic Latent Gaussian Models,” in Interna-\ntional Conference on Machine Learning, pp. 633–640. [872]\nMcGrory, C. A., and Titterington, D. M. (2007), “Variational Approx-\nimations in Bayesian Model Selection for Finite Mixture Dis-\ntributions,”\nComputational\nStatistics\nand\nData\nAnalysis,\n51,\n5352–5367. [862]\nMetropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, M., and Teller, E.\n(1953), “Equations of State Calculations by Fast Computing Machines,”\nJournal of Chemical Physics, 21, 1087–1092. [859]\nMinka, T. P. (2001), “Expectation Propagation for Approximate Bayesian\nInference,” in Uncertainty in Artificial Intelligence, pp. 362–369.\n[860,873]\n——— (2005), “Divergence Measures and Message Passing,” Technical\nReport, Microsoft Research. [864,873]\nMinka, T., Winn, J., Guiver, J., Webster, S., Zaykov, Y., Yangel, B., Spengler,\nA., and Bronskill, J. (2014), Infer.NET 2.6. Cambridge, MA: Microsoft\nResearch. [864]\nNaseem, T., Chen, H., Barzilay, R., and Johnson, M. (2010), “Using Univer-\nsal Linguistic Knowledge to Guide Grammar Induction,” in Empirical\nMethods in Natural Language Processing, pp. 1234–1244. [871]\nNathoo, F., Babul, A., Moiseev, A., Virji-Babul, N., and Beg, M. (2014),\n“A Variational Bayes Spatiotemporal Model for Electromagnetic Brain\nMapping,” Biometrics, 70, 132–143. [871]\nNeal, R. M., and Hinton, G. E. (1998), “A View of the EM Algorithm that\nJustifies Incremental, Sparse, and other Variants,” in Learning in Graph-\nical Models, New York: Springer, pp. 355–368. [860]\nNeville, S., Ormerod, J., and Wand, M. (2014), “Mean Field Variational\nBayes for Continuous Sparse Signal Shrinkage: Pitfalls and Remedies,”\nElectronic Journal of Statistics, 8, 1113–1151. [872]\nNott, D. J., Tan, S. L., Villani, M., and Kohn, R. (2012), “Regres-\nsion Density Estimation with Variational Methods and Stochastic\nApproximation,” Journal of Computational and Graphical Statistics, 21,\n797–820. [862,873]\nOpper, M., and Winther, O. (2005), “Expectation Consistent Approx-\nimate Inference,” The Journal of Machine Learning Research, 6,\n2177–2204. [873]\nOrmerod, J., You, C., and Muller, S. (2014), “A Variational Bayes\nApproach to Variable Selection,” unpublished manuscript, avail-\nable at http://www.maths.usyd.edu.au/u/jormerod/JTOpapers/Variab-\nleSelectionFinal.pdf. [872]\nPaisley, J., Blei, D., and Jordan, M. I. (2012), “Variational Bayesian Inference\nwith Stochastic Search,” in Proceedings of the 29th International Confer-\nence on International Conference on Machine Learning, Madison, WI:\nOmnipress, pp. 1363–1370. [873]\nParisi, G. (1988), Statistical Field Theory, Melville, NY: Addison-Wesley.\n[860]\nPearl, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of\nPlausible Inference, San Francisco, CA: Morgan Kaufmann. [864]\nPenny, W., Kiebel, S., and Friston, K. (2003), “Variational Bayesian Inference\nfor fMRI Time Series,” NeuroImage, 19, 727–741. [871]\nPenny, W., Trujillo-Barreto, N., and Friston, K. (2005), “Bayesian\nfMRI Time Series Analysis with Spatial Priors,” Neuroimage, 24,\n350–362. [871]\nPeterson, C., and Anderson, J. (1987), “A Mean Field Theory Learning\nAlgorithm for Neural Networks,” Complex Systems, 1, 995–1019. [860]\nRaj, A., Stephens, M., and Pritchard, J. (2014), “fastSTRUCTURE: Vari-\national Inference of Population Structure in Large SNP Data Sets,”\nGenetics, 197, 573–589. [871]\n876\nD. M. BLEI, A. KUCUKELBIR, AND J. D. MCAULIFFE\nRamos, F., Upcroft, B., Kumar, S., and Durrant-Whyte, H. (2012), “A\nBayesian Approach for Place Recognition,” Robotics and Autonomous\nSystems, 60, 487–497. [871]\nRanganath, R., Gerrish, S., and Blei, D. (2014), “Black Box Variational Infer-\nence,” in Artificial Intelligence and Statistics, pp. 814–822. [873]\nRanganath, R., Tran, D., and Blei, D. (2016), “Hierarchical Variational Mod-\nels,” in International Conference on Machine Learning, pp. 324–333.\n[873]\nRegier, J., Miller, A., McAuliffe, J., Adams, R., Hoffman, M., Lang, D.,\nSchlegel, D., and Prabhat (2015), “Celeste: Variational Inference for a\nGenerative Model of Astronomical Images,” in International Conference\non Machine Learning, pp. 2095–2103. [872]\nReyes-Gomez, M., Ellis, D., and Jojic, N. (2004), “Multiband Audio Mod-\neling for Single-Channel Acoustic Source Separation,” in Acoustics,\nSpeech, and Signal Processing, pp. 641–644. [872]\nRezende, D. J., Mohamed, S., and Wierstra, D. (2014), “Stochastic Back-\npropagation and Approximate Inference in Deep Generative Models,”\nin Proceedings of the 31st International Conference on Machine Learning\n(Vol. 32), eds. E. P. Xing and T. Jebara, Beijing, China: Proceedings of\nMachine Learning Research, pp. 1278–1286. [873]\nRobbins, H., and Monro, S. (1951), “A Stochastic Approximation Method,”\nThe Annals of Mathematical Statistics, 22, 400–407. [860,869,870]\nRobert, C., and Casella, G. (2004), Monte Carlo Statistical Methods (Springer\nTexts in Statistics), New York: Springer-Verlag. [859,860]\nRoberts, S., Guilford, T., Rezek, I., and Biro, D. (2004), “Positional Entropy\nDuring Pigeon Homing I: Application of Bayesian Latent State Mod-\nelling,” Journal of Theoretical Biology, 227, 39–50. [872]\nRoberts, S., and Penny, W. (2002), “Variational Bayes for Generalized\nAutoregressive Models,” IEEE Transactions on Signal Processing, 50,\n2245–2257. [871]\nRohde, D., and Wand, M. (2016), “Semiparametric Mean Field Variational\nBayes: General Principles and Numerical Issues,” Journal of Machine\nLearning Research, 17, 1–47. [873]\nSalimans, T., Kingma, D., and Welling, M. (2015), “Markov Chain Monte\nCarlo and Variational Inference: Bridging the Gap,” in International\nConference on Machine Learning, pp. 1218–1226. [873]\nSalimans, T., and Knowles, D. (2014), “On using Control Variates with\nStochastic Approximation for Variational Bayes,” arXiv preprint,\narXiv:1401.1022. Available at https://arxiv.org/abs/1401.1022 [873]\nSanguinetti, G., Lawrence, N., and Rattray, M. (2006), “Probabilistic Infer-\nence of Transcription Factor Concentrations and Gene-Specific Regu-\nlatory Activities,” Bioinformatics, 22, 2775–2781. [871]\nSato, M. (2001), “Online Model Selection Based on the Variational Bayes,”\nNeural Computation, 13, 1649–1681. [870]\nSato, M., Yoshioka, T., Kajihara, S., Toyama, K., Goda, N., Doya, K., and\nKawato, M. (2004), “Hierarchical Bayesian Estimation for MEG Inverse\nProblem,” NeuroImage, 23, 806–826. [871]\nSaul, L., and Jordan, M. I. (1996), “Exploiting Tractable Substructures\nin Intractable Networks,” in Neural Information Processing Systems,\npp. 486–492. [863,873]\nSaul, L. K., Jaakkola, T., and Jordan, M. I. (1996), “Mean Field Theory for\nSigmoid Belief Networks,” Journal of Artificial Intelligence Research, 4,\n61–76. [860]\nSpall, J. (2003), Introduction to Stochastic Search and Optimization: Estima-\ntion, Simulation, and Control, New York: Wiley. [870]\nStan Development Team (2015), Stan Modeling Language Users Guide\nand Reference Manual, Version 2.8.0. New York: Columbia University.\n[866,873]\nStegle, O., Parts, L., Durbin, R., and Winn, J. (2010), “A Bayesian Framework\nto Account for Complex Non-Genetic Factors in Gene Expression Lev-\nels Greatly Increases Power in eqtl Studies,” PLoS Computational Biol-\nogy, 6, e1000770. [871]\nSudderth, E. B., and Jordan, M. I. (2009), “Shared Segmentation of Natural\nScenes using Dependent Pitman-Yor Processes,” in Neural Information\nProcessing Systems, pp. 1585–1592. [871]\nSung, J., Ghahramani, Z., and Bang, Y. (2008), “Latent-Space Variational\nBayes,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\n30, 2236–2242. [864]\nSykacek, P., Roberts, S., and Stokes, M. (2004), “Adaptive BCI Based on\nVariational Bayesian Kalman Filtering: An Empirical Evaluation,” IEEE\nTransactions on Biomedical Engineering, 51, 719–727. [871]\nTan, L., and Nott, D. (2013), “Variational Inference for Generalized Linear\nMixed Models using Partially Noncentered Parametrizations,” Statisti-\ncal Science, 28, 168–188. [873]\n——— (2014), “A Stochastic Variational Framework for Fitting and Diag-\nnosing Generalized Linear Mixed Models,” Bayesian Analysis, 9, 963–\n1004. [873]\n——— (2017), “Gaussian Variational Approximation with Sparse Precision\nMatrix,” Statistics and Computing, 1–17. [873]\nTipping, M., and Lawrence, N. (2005), “Variational Inference for Student-\nt models: Robust Bayesian Interpolation and Generalised Component\nAnalysis,” Neurocomputing, 69, 123–141. [872]\nTitsias, M., and Lawrence, N. (2010), “Bayesian Gaussian Process\nLatent Variable Model,” in Artificial Intelligence and Statistics,\npp. 844–851. [872]\nTitsias, M., and Lázaro-Gredilla, M. (2014), “Doubly Stochastic Variational\nBayes for Non-Conjugate Inference,” in International Conference on\nMachine Learning, pp. 1971–1979. [873]\nTran, D., Ranganath, R., and Blei, D. M. (2016), “The Variational Gaus-\nsian Process,” in International Conference on Learning Representations,\npp. 1–4. [873]\nUeda, N., and Ghahramani, Z. (2002), “Bayesian Model Search for Mixture\nModels Based on Optimizing Variational Bounds,” Neural Networks,\n15, 1223–1241. [862]\nVan Den Broek, B., Wiegerinck, W., and Kappen, B. (2008), “Graphi-\ncal Model Inference in Optimal Control of Stochastic Multi-Agent\nSystems,” Journal of Artificial Intelligence Research, 32, 95–122.\n[872]\nVermaak, J., Lawrence, N. D., and Pérez, P. (2003), “Variational Infer-\nence for Visual Tracking,” in Computer Vision and Pattern Recognition,\npp. 1–8. [871]\nVillegas, M., Paredes, R., and Thomee, B. (2013), “Overview of the Image-\nCLEF 2013 Scalable Concept Image Annotation Subtask,” in CLEF\nEvaluation Labs and Workshop, pp. 308–328. [866]\nWainwright, M. J., and Jordan, M. I. (2008), “Graphical Models, Exponen-\ntial Families, and Variational Inference,” Foundations and Trends in\nMachine Learning, 1, 1–305. [859,860]\nWand, M. (2014), “Fully Simplified Multivariate Normal Updates in Non-\nConjugate Variational Message Passing,” Journal of Machine Learning\nResearch, 15, 1351–1369. [873]\nWand, M., Ormerod, J., Padoan, S., and Fuhrwirth, R. (2011), “Mean Field\nVariational Bayes for Elaborate Distributions,” Bayesian Analysis, 6,\n847–900. [864,872]\nWang, B., and Titterington, D. (2005), “Inadequacy of Interval Estimates\nCorresponding to Variational Bayesian Approximations,” in Artificial\nIntelligence and Statistics, pp. 373–380. [872]\n——— (2006), “Convergence Properties of a General Algorithm for Cal-\nculating Variational Bayesian Estimates for a Normal Mixture Model,”\nBayesian Analysis, 1, 625–650. [872]\nWang, C., and Blei, D. (2013), “Variational Inference in Nonconju-\ngate Models,” Journal of Machine Learning Research, 14, 1005–1031.\n[872]\n——— (2015), “A General Method for Robust Bayesian Modeling,” Journal\nof Machine Learning Research, 14, 1005–1031. [872]\nWang, P., and Blunsom, P. (2013), “Collapsed Variational Bayesian Inference\nfor Hidden Markov Models,” in Artificial Intelligence and Statistics, pp.\n599–607. [871]\nWang, Y., and Mori, G. (2009), “Human Action Recognition by Semila-\ntent Topic Models,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 31, 1762–1774. [871]\nWaterhouse, S., MacKay, D., and Robinson, T. (1996), “Bayesian Methods\nfor Mixtures of Experts,” in Neural Information Processing Systems, pp.\n351–357. [860]\nWelling, M., and Teh, Y. (2011), “Bayesian Learning via Stochastic Gradient\nLangevin Dynamics,” in International Conference on Machine Learning,\npp. 681–688. [860]\nWestling, T., and McCormick, T. H. (2015), “Establishing Consis-\ntency and Improving Uncertainty Estimates of Variational Inference\nThrough M-estimation,” arXiv preprint, arXiv:1510.08151. Available at\nhttps://arxiv.org/abs/1510.08151 [872]\nWiggins, C., and Hofman, J. (2008), “Bayesian Approach to Network Mod-\nularity,” Physical Review Letters, 100, 258701. [872]\nJOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION\n877\nWingate, D., and Weber, T. (2013), “Automated Variational Inference in\nProbabilistic Programming,” arXiv preprint, arXiv:1301.1299. Avail-\nable at https://arxiv.org/abs/1301.1299 [873]\nWinn, J., and Bishop, C. (2005), “Variational Message Passing,” Journal of\nMachine Learning Research, 6, 661–694. [863]\nWipf, D., and Nagarajan, S. (2009), “A Unified Bayesian Framework for\nMEG/EEG Source Imaging,” NeuroImage, 44, 947–966. [871]\nWoolrich, M., Behrens, T., Beckmann, C., Jenkinson, M., and Smith, S.\n(2004), “Multilevel Linear Modeling for fMRI Group Analysis using\nBayesian Inference,” Neuroimage, 21, 1732–1747. [871]\nXing, E., Wu, W., Jordan, M. I., and Karp, R. (2004), “Logos: A Modular\nBayesian Model for de novo motif Detection,” Journal of Bioinformatics\nand Computational Biology, 2, 127–154. [871]\nYedidia, J. S., Freeman, W. T., and Weiss, Y. (2001), “Generalized Belief Prop-\nagation,” in Neural Information Processing Systems, pp. 689–695. [860]\nYogatama, D., Wang, C., Routledge, B., Smith, N. A., and Xing, E.\n(2014), “Dynamic Language Models for Streaming Text,” Transac-\ntions of the Association for Computational Linguistics, 2, 181–192.\n[871]\nYou, C., Ormerod, J., and Muller, S. (2014), “On Variational Bayes\nEstimation and Variational Information Criteria for Linear Regres-\nsion Models,” Australian & New Zealand Journal of Statistics, 56,\n73–87. [872]\nYu, T., and Wu, Y. (2005), “Decentralized Multiple Target Tracking using\nNetted Collaborative Autonomous Trackers,” in Computer Vision and\nPattern Recognition, pp. 939–946. [871]\nZumer, J., Attias, H., Sekihara, K., and Nagarajan, S. (2007), “A\nProbabilistic Algorithm Integrating Source Localization and Noise\nSuppression for MEG and EEG Data,” NeuroImage, 37, 102–115.\n[871]\n"
